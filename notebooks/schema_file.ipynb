{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFACTOR FILE SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you've already completed the instructions on the **Installation** page, then let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping aiqc as it is not installed.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall aiqc -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/layne/Desktop/aiqc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/layne/.pyenv/versions/3.7.6/envs/jupyterlab/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import aiqc\n",
    "from aiqc import examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Success - deleted database file at path:\n",
      "/Users/layne/Library/Application Support/aiqc/aiqc.sqlite3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aiqc.delete_db(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'aiqc' from '/Users/layne/Desktop/aiqc/aiqc/__init__.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(aiqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Success - created database file for machine learning metrics at path:\n",
      "/Users/layne/Library/Application Support/aiqc/aiqc.sqlite3\n",
      "\n",
      "\n",
      "=> Success - created the following tables within database:\n",
      "['algorithm', 'batch', 'datapipeline', 'dataset', 'experiment', 'featureset', 'file', 'fold', 'foldset', 'hyperparamcombo', 'hyperparamset', 'image', 'job', 'label', 'preprocess', 'result', 'splitset', 'tabular']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aiqc.create_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([\n",
    "    np.array(['a']),\n",
    "    np.array(['b']),\n",
    "    np.array(['c']),\n",
    "    np.array(['d']),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tab = aiqc.Dataset.Tabular.from_pandas(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'notebooks/demo_pics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = aiqc.Dataset.Image.from_folder(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = aiqc.Dataset.Image.from_folder(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make a `Fileset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Fileset` consists of one or more files that are made from either:\n",
    "* In-memory data structures like Pandas DataFrame and NumPy Array\n",
    "* Files like CSV and TSV.\n",
    "\n",
    "This ensures that your data is: persistent, immutable, compressed, and transferable - all of which greatly increases reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `perform_gzip=True` will reduce the size of the large files by ~25% to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = examples.get_demo_file_path('iris.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiqc.Dataset.Tabular.from_path(file_path=path, file_format='tsv', column_names=['a','b','c','d','e'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a File or Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**; sometimes files either have no column names or you want to provide your own column names. To make it easy to deal with these scenarios, we expose the following arguments: \n",
    "- `column_names` behaves exactly the same as `Pandas.read_csv(names)`.\n",
    "- `skip_header_rows`  exactly the same as `Pandas.read_csv(header)`.\n",
    "\n",
    "> For more detail on things like overriding column names, visit the Pandas docs https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Numpy Array(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = examples.demo_file_to_pandas('iris.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiqc.Dataset.Tabular.from_pandas(dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular *ndarrays* don't have column names, and I didn't like the API for *structured arrays* so you have to pass in columns names as a list. If you don't then column names will be numerically assigned in ascending order (zero-based index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr =  df.to_numpy()\n",
    "cols = list(df.columns)\n",
    "\n",
    "file = aiqc.File.from_numpy(\n",
    "\tndarray = arr\n",
    "\t, file_format = 'parquet'\n",
    "\t, name = 'chunking plants'\n",
    "\t, perform_gzip = True\n",
    "\t, column_names = cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_file_path = examples.get_demo_file_path('iris_10x.tsv')\n",
    "\n",
    "# we'll keep this one handy for later.\n",
    "big_dataset = aiqc.Dataset.from_file(\n",
    "\tpath = demo_file_path # files must have column names as their first row\n",
    "\t, file_format = 'tsv'\n",
    "\t, perform_gzip = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The bytes of the data will be stored as a BlobField in the SQLite database file. Storing the data in the database not only (a) provides an entity that we can use to keep track of experiments and link relational data to but also (b) makes the data less mutable than keeping it in the open filesystem.\n",
    "\n",
    "> You can choose whether or not you want to gzip compress the file when importing it with the `perform_gzip=bool` parameter. This compression not only enables you to store up to 90% more data on your local machine, but also helps overcome the maximum BlobField size of 2.147 GB. We handle the zipping and unzipping on the fly for you, so you don't even notice it.\n",
    "\n",
    "> Optionally, `dtype`, as seen in [`pandas.DataFrame.astype(dtype)`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html), can be specified as either a single type for all columns, or as a dict that maps a specific type to each column name. This encodes features for analysis. We read NumPy into Pandas before persisting it, so `columns` and `dtype` are read directly by `pd.DataFrame()`.\n",
    "\n",
    "> At this point, the project's support for Parquet is extremely minimal.\n",
    "\n",
    "> If you leave `name` blank, it will default to a human-readble timestamp with the appropriate file extension (e.g. '2020_10_13-01_28_13_PM.tsv')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch a `Dataset` with either **Pandas** or **NumPy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data-oriented objects in the API have `to_numpy()` and `to_pandas()` methods that accept the following arguments:\n",
    "\n",
    "* `samples=[]` list of indeces to fetch.\n",
    "* `columns=[]` list of columns to fetch.\n",
    "\n",
    "Later, we'll see how these arguments allow downstream objects like `Splitset` and `Foldset` to slice up the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = dataset.to_numpy(\n",
    "    samples = [0,13,29,79] \n",
    "    , columns = ['petal_length', 'petal_width']\n",
    ")\n",
    "arr[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aiqc.Dataset.to_pandas(\n",
    "    id = dataset.id \n",
    "    , samples = [0,13,29,79]\n",
    "    , columns = ['sepal_length', 'sepal_width']\n",
    ")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = aiqc.Dataset.to_numpy(id=dataset.id)\n",
    "arr[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select the `Label` column(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Dataset, pick the column(s) that you want to train against/ predict. If you are planning on training an unsupervised model, then you don't need to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `Label` won't duplicate your data! It simply records the `columns` to be used for supervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'species'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset.make_label(columns=[label_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = aiqc.Label.from_dataset(\n",
    "\tdataset_id=1 # cannot duplicate labels on the same dataset\n",
    "\t, columns=[label_column]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `columns=[label_column]` is a list in case we want to do something with raw OHE/ tensors in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch a `Label` with either **Pandas** or **NumPy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Label` comes in handy when we need to fetch *Y* splits. It accepts a `samples` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     species\n",
       "145        2\n",
       "146        2\n",
       "147        2\n",
       "148        2\n",
       "149        2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.to_pandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.to_numpy(samples=[0,33,66,99,132])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Select the `Featureset` column(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Featureset won't duplicate your data! It simply records the `columns` to be used in training. \n",
    "\n",
    "There are three ways to define which columns you want to use as features:\n",
    "\n",
    "- `exclude_columns=[]` e.g. use all columns except the label column.\n",
    "- `include_columns=[]` e.g. only use these columns that I think are informative.\n",
    "- Leave both of the above blank and all columns will be used e.g. unsupervised leanring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit IDs w `exclude_columns=[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = dataset.make_featureset(exclude_columns=[label_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit IDs w `include_columns=[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_columns = [\n",
    "    'sepal_length',\n",
    "    'petal_length',\n",
    "    'petal_width'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = aiqc.Featureset.from_dataset(\n",
    "\tdataset_id = 1 # cannot duplicate featureset on a dataset\n",
    "\t, include_columns = include_columns\n",
    "\t, exclude_columns = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal_length', 'petal_length', 'petal_width']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal_width', 'species']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.columns_excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch a `Featureset` with either **Pandas** or **NumPy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2],\n",
       "       [4.6, 1.5, 0.2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.to_numpy()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal_length  petal_length  petal_width\n",
       "0            5.1           1.4          0.2\n",
       "16           5.4           1.3          0.4\n",
       "32           5.2           1.5          0.1\n",
       "64           5.6           3.6          1.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.to_pandas(samples=[0,16,32,64]).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Slice samples with a `Splitset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Splitset` divides a the samples of the Dataset into the following *splits*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Split                 | Description                                                                                                                                                                                             |\n",
    "|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| train                 | The samples that the model will be trained upon. <br/>Later, weâ€™ll see how we can make cross-folds from our training split. <br/>Unsupervised learning will only have a training split.                 |\n",
    "| validation (optional) | The samples used for training evaluation. <br/>Ensures that the test set is not revealed to the model during training.                                                                                  |\n",
    "| test (optional)       | The samples the model has never seen during training. <br/>Used to assess how well the model will perform on unobserved, natural data when it is applied in the real world aka how generalizable it is. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, creating a Splitset won't duplicate your data. It simply records the sample indeces (aka rows) to be used in the splits that you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ways to split a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Default supervised 70-30 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only provide a Label, then 70:30 train:test splits will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = featureset.make_splitset(label_id=label.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Specifying test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = featureset.make_splitset(\n",
    "\tlabel_id = label.id\n",
    "\t, size_test = 0.30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Specifying validation size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = featureset.make_splitset(\n",
    "\tlabel_id = label.id\n",
    "\t, size_test = 0.20\n",
    "\t, size_validation = 0.12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) Taking the whole dataset as a training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset_unsupervised = featureset.make_splitset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Label-based stratification is used to ensure equally distributed label classes for both categorical and continuous data.\n",
    ">\n",
    "> If you want more control over stratification of continuous splits, specify the number of `continuous_bin_count` for grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the actual size of your splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset.sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching a `Splitset` into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where things start to get interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given that there are potentially multiple splits, `Splitset` methods return dictionaries where each entry corresponds with a split.\n",
    "  * Additionally, each split will contain *features* and, potentially, *labels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset.to_numpy()['train']['features'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset.to_pandas()['test']['labels'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optionally, create a `Foldset` for cross-fold validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reference the [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html) to learn more about folding.*\n",
    "\n",
    "![Cross Folds](../images/cross_fold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the left out fold as the `fold_validation` and the remaining training data as the `folds_train_combined`. The sample indeces of the validation fold are still recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Don't use `fold_count` unless your (total sample count / fold_count) still gives you an accurate representation of your sample population.\n",
    "\n",
    "> In a scenario where a validation split was specified in the original Splitset, the validation split is not included in the Folds. Only the training data is folded. The implication is that you can have 2 validations in the form of the validation split and the validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-folding takes many samples, especially when stratified.\n",
    "# which is why we set aside the 'big_dataset' made from 'iris_10x.tsv' earlier.\n",
    "big_label = big_dataset.make_label(columns=[label_column])\n",
    "big_fset = big_dataset.make_featureset(exclude_columns=[label_column])\n",
    "big_splits = big_fset.make_splitset(\n",
    "\tlabel_id = big_label.id\n",
    "\t, size_test = 0.30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates 5 `Fold` objects that belong to the `Foldset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset = big_splits.make_foldset(fold_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Fold` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of determining which samples get trained upon, the only thing that matters is the slice of data that gets left out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took a slightly simplified approach in that each `Fold` has a dictionary that contains:\n",
    "* `samples['folds_train_combined']` - all the included folds.\n",
    "* `samples['fold_validation']` - the fold that got left out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross fold objects](../images/cross_fold_objects.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(foldset.folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample indeces of each Fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset.folds[0].samples['folds_train_combined'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset.folds[0].samples['fold_validation'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching a `Foldset` into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce memory footprint the `to_numpy()` and `to_pandas()` methods introduce the `fold_index` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no fold_index is specified, then it will fetch all folds and give each fold a numeric key according to its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset.to_numpy().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "So you need to specify the `fold_index` as the first key when accessing the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset.to_numpy(fold_index=0)[0]['fold_validation']['features'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset.to_pandas(fold_index=0)[0]['folds_train_combined']['labels'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optionally, create a `Preprocess` for features & labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain algorithms need features and/ or labels formatted a certain way. For example, converting categorical data `[dog, cat, fish]` to one-hot encoded format `[[1,0,0][0,1,0][0,0,1]]`.\n",
    "\n",
    "The tricky thing about preprocessing is that you are supposed to `fit` it to the training data (train, folds_train_combined), and then `transform` each of the other splits (fold_validation, validation, test) in order to avoid bias. So the dataset itself should not be stored in preprocessed format.\n",
    "\n",
    "So you simply defined the encoders that you want to use and then they will automatically be applied to the appropriate splits/ folds during training.\n",
    "\n",
    "> For now, only `sklearn.preprocessing` methods are supported. That may change as we add support for more low-level tensor-based frameworks. And if people want to be able to run multiple encoders on their features of different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_features = StandardScaler()\n",
    "encoder_labels = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = aiqc.Preprocess.from_splitset(\n",
    "    splitset_id = splitset.id\n",
    "    , description = \"standard scaling on features\"\n",
    "    , encoder_features = encoder_features\n",
    "    , encoder_labels = encoder_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create an `Algorithm` aka model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Algorithm` is the ORM's codename for a machine learning model since *Model* is the most important *reserved word* for ORMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define functions to **build** and **train** our model.\n",
    "\n",
    "You can name the functions whatever you want, but do not change the predetermined `*args` (e.g. `**hyperparameters`, `model`, etc.).\n",
    "\n",
    "Put a placeholder anywhere you want to try out different hyperparameters: `hyperparameters['<some_variable_name>']`. You'll get a chance to define the hyperparameters in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Algorithm` is the ORM's codename for a machine learning model since *Model* is the most important *reserved word* for ORMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_model_build(**hyperparameters):\n",
    "    \n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(hyperparameters['neuron_count'], input_shape=(3,), activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(hyperparameters['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\topt = keras.optimizers.Adamax(hyperparameters['learning_rate'])\n",
    "\tmodel.compile(\n",
    "\t\tloss = 'categorical_crossentropy'\n",
    "\t\t, optimizer = opt\n",
    "\t\t, metrics = ['accuracy']\n",
    "\t)\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_model_train(model, samples_train, samples_evaluate, **hyperparameters):\n",
    "    \n",
    "\tmodel.fit(\n",
    "\t\tsamples_train[\"features\"]\n",
    "\t\t, samples_train[\"labels\"]\n",
    "\t\t, validation_data = (\n",
    "\t\t\tsamples_evaluate[\"features\"]\n",
    "\t\t\t, samples_evaluate[\"labels\"]\n",
    "\t\t)\n",
    "\t\t, verbose = 0\n",
    "\t\t, batch_size = 3\n",
    "\t\t, epochs = hyperparameters['epoch_count']\n",
    "\t\t, callbacks=[History()]\n",
    "\t)\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional, functions to predict samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating an `Algorithm`, the predict function will be generated for you automatically if set to `None`.\n",
    "\n",
    "> The `analysis_type` and `library` of the Algorithm help determine how to handle the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_model_predict(model, samples_predict):\n",
    "    predictions = model.predict(samples_predict['features'])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Classification binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classification `predictions`, both mutliclass and binary, must be returned in ordinal format. \n",
    "\n",
    "> For most libraries, classification algorithms output probabilities as opposed to actual predictions when running `model.predict()`. We want to return both of these object `predictions, probabilities` (the order matters) to generate performance metrics behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_model_predict(model, samples_predict):\n",
    "    probabilities = model.predict(samples_predict['features'])\n",
    "    # This is the official keras replacement for binary classes `.predict_classes()`\n",
    "    # It returns one array per sample: `[[0][1][0][1]]` \n",
    "    predictions = (probabilities > 0.5).astype(\"int32\")\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Classification multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_model_predict(model, samples_predict):\n",
    "    probabilities = model.predict(samples_predict['features'])\n",
    "    # This is the official keras replacement for multiclass `.predict_classes()`\n",
    "    # It returns one ordinal array per sample: `[[0][2][1][2]]` \n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional, functions to calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating an `Algorithm`, the evaluate function will be generated for you automatically if set to `None`.\n",
    "\n",
    "> The `analysis_type` and `library` of the Algorithm help determine how to handle the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only trick thing here is when `keras.metrics` returns multiple metrics, like *accuracy* or *R^2*. All we are after in this case is the loss for the split/ fold in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_model_loss(model, samples_evaluate):\n",
    "    metrics = model.evaluate(samples_evaluate['features'], samples_evaluate['labels'], verbose=0)\n",
    "    if (isinstance(metrics, list)):\n",
    "        loss = metrics[0]\n",
    "    elif (isinstance(metrics, float)):\n",
    "        loss = metrics\n",
    "    else:\n",
    "        raise ValueError(f\"\\nYikes - The 'metrics' returned are neither a list nor a float:\\n{metrics}\\n\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In contrast to openly specifying a loss function, for example `keras.losses.<loss_fn>()`, the use of `.evaluate()` is consistent because it comes from the compiled model. Also, although `model.compiled_loss` would be more efficient, it requires making encoded `y_true` and `y_pred` available to the user, whereas `.evaluate()` can be called with the same arugments as the other `function_model_*` and many deep learning libraries support this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group the functions together in an `Algorithm`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = aiqc.Algorithm.create(\n",
    "    library = \"keras\"\n",
    "\t, analysis_type = \"classification_multi\"\n",
    "\t, function_model_build = function_model_build\n",
    "\t, function_model_train = function_model_train\n",
    "\t, function_model_predict = function_model_predict\n",
    "\t, function_model_loss = function_model_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Optional, associate `hyperparameters` with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hyperparameters` below will be automatically fed into the functions above as `**kwargs` via the `**hyperparameters` argument we saw earlier.\n",
    "\n",
    "For example, wherever you see `hyperparameters['neuron_count']`, it will pull from the *key:value* pair `\"neuron_count\": [9, 12]` seen below. Where model A will have 9 neurons and model B will have 12 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "\t\"neuron_count\": [9, 12]\n",
    "\t, \"epoch_count\": [30, 60]\n",
    "    , \"learning_rate\": [0.03, 0.05]\n",
    "}\n",
    "\n",
    "hyperparamset = aiqc.Hyperparamset.from_algorithm(\n",
    "\talgorithm_id = algorithm.id\n",
    "\t, description = \"experimenting with neuron count, layers, and epoch count\"\n",
    "\t, hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the future, we will provide different strategies for generating and selecting parameters to experiment with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Hyperparamcombo` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unique combination of hyperparameters is recorded. A separate training `Job` will be made for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamset.hyperparamcombo_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamcombos = hyperparamset.hyperparamcombos\n",
    "\n",
    "for h in hyperparamcombos:\n",
    "    print(h.hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Create a `Batch` of `Jobs` to keep track of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Batch` ties together everything you need for hypertuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = aiqc.Batch.from_algorithm(\n",
    "\talgorithm_id = algorithm.id\n",
    "\t, splitset_id = splitset.id\n",
    "\t, hyperparamset_id = hyperparamset.id\n",
    "\t, foldset_id = None\n",
    "\t, preprocess_id = preprocess.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Job` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Job` in the Batch represents a training run. It contains the information needed to execute the training run. Its `status` keeps track of its phase of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.jobs[0].status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.get_statuses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the `Batch`.\n",
    "The Jobs will be asynchronously executed on a background process, so that you can continue to code on the main process. You can poll the Job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the execution of a batch if you need to, and later resume it. If your kernel crashes then you can likewise resume the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.stop_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.get_statuses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before resuming them again. In this way, if your system crashes for any reason, you can pick up right back where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Assess the `Results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Job has a `Result`. The following artifacts are automatically written to the Result after training.\n",
    "    \n",
    "* `model_file`: hdf5 bytes of the model.\n",
    "* `history`: per epoch metrics recorded during training.\n",
    "* `predictions`: dictionary of predictions per split/ fold.\n",
    "* `probabilities`: dictionary of prediction probabilities per split/ fold.\n",
    "* `metrics`: dictionary of single-value metrics depending on the analysis_type.\n",
    "* `plot_data`: metrics readily formatted for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the dictionaries have split/ fold based keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = batch.jobs[0].results[0].get_model()\n",
    "compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.jobs[0].results[0].metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information of visualization of performance metrics, reference the **Visualization & Metrics** documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
