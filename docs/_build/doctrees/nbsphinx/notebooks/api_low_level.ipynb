{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_static/images/banner/sql.png\" class=\"banner-photo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-Relational Model (ORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Low-Level API is an *object-relational model* for machine learning. Each class in the [ORM](http://docs.peewee-orm.com/en/latest/peewee/models.html) maps to a table in a SQLite database that serves as a machine learning *metastore*. \n",
    "\n",
    "The real power lies in the relationships between these objects (e.g. `Label`→`Splitset`←`Feature` and `Queue`→`Job`→`Predictor`→`Prediction`), which enable us to construct rule-base protocols for various types of data and analysis.\n",
    "\n",
    "Goobye, *X_train, y_test*. Hello, object-oriented machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from aiqc.orm import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Datasets](../_static/images/api/dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` class provides the following subclasses for working with different types of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type | Dimensionality | Supported Formats | Format (if ingested)\n",
    "--- | --- | --- | ---\n",
    "**Tabular** | 2D | Files (Parquet, CSV, TSV) / Pandas DataFrame (in-memory) | Parquet\n",
    "**Sequence** | 3D | NumPy (in-memory ndarray, npy file) | npy \n",
    "**Image** | 4D | NumPy (in-memory ndarray, npy file) / Pillow-supported formats | npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The names are merely suggestive, as the primary purpose of these subclasses is to provide a way to register data of known dimensionality.  For example, a practitioner could ingest many uni-channel/ grayscale images as a 3D Sequence Dataset instead of a multi-channel 4D Image Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Why not 2D NumPy?* The `Dataset.Tabular` class is intended for strict, column-specific dtypes and Parquet persistence upon ingestion. In practice, this conflicted too often with NumPy's array-wide dtyping. We use the best tools for the job (df/pq for 2D) and (array/npy for ND)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Most of the Dataset registration methods share these arguments/ concepts:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Description\n",
    "--- | ---\n",
    "**ingest** | Determines if raw data is either stored directly inside the metastore or remains on disk to be accessed via path/url. *In-memory* data like DataFrames and ndarrays must be ingested. Whereas *file-based* data like Parquet, NPY, Image folders/urls may remain remote. Regardless of whether or not the raw data is ingested, metadata is always derived from it by parsing: 2D via DataFrame and N-D via ndarray.\n",
    "**rename_columns** | Useful for assigning column names to arrays or delimited files that would otherwise be unnamed. `len(rename_columns)` must match the number of columns in the raw data. Normally, an int-based range is assigned to unnamed columns. In this case, AIQC converts each column name to a string e.g. '1' during the registration process.\n",
    "**retype** | Change the dtype of data using [np.types](https://numpy.org/doc/stable/user/basics.types.html). All Dataset subclasses support mass typing via `np.type`/ `str(np.type)`. Only the Tabular subclass supports inidividual column retyping via `dict(column=str(np.type))`. If `rename_columns` is used in conjuction with `retype=dict()`, then each `dict['column']` key must match its counterpart in rename_columns.\n",
    "**description** | What information does this dataset contain? What is unique about this dataset/ version -- did you edit the raw data, add rows, or change column names/ dtypes?\n",
    "**name** | Triggers dataset *versioning*. Datasets that share a name will be assigned an auto-incrementing `version:int` number provided that they are not duplicates of each other based on a `sha256_hexdigest:str` hash. If you try to create an exact duplicate, it will warn you and `return` the matching duplicate instead of creating a new entity. This behavior makes it easy to rerun pipelines where Datasets are created inline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ingestion provides the following benefits, especially for entry-level users:*\n",
    "\n",
    "- Persist in-memory datasets (Pandas DataFrames, NumPy ndarrays).\n",
    "- Keeps data coupled with the experiment in the portable SQLite file.\n",
    "- Provides a more immutable and out-of-the-way storage location in comparison to a laptop file system.\n",
    "- Encourages preserving tabular dtypes with the ecosystem-friendly Parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why would I avoid ingestion?*\n",
    "\n",
    "- Happy with where the original data lives: e.g. S3 bucket.\n",
    "- Don't want to duplicate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *sha256?* -- It's the one-way hash algorithm that GitHub aspires to upgrade to. AIQC runs it on compressed data because it's easier and probably less-error prone than intercepting the bytes of the *fastparquet* intermediary tables before appending the Parquet magic bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Is SQLite a legitimate datastore?* -- In many cases, SQLite queries are faster than accessing data via a filesystem. It's a stable, 22 year-old technology that serves as the default database for iOS e.g. Apple Photos. AIQC uses it store raw data in byte format as a BlobField. I've stored tens-of-thousands of files in it over several years and never experienced corruption. Keep in mind that AWS S3 is blob store, and the Microsoft equivalent service is literally called Azure *Blob* Storage. The max size of a BlobField is 2GB, so ~20GB after compression. Either way, the goal of machine learning isn't to record the entire population within the weights of a neural network, it's to find subsets that are representative of the broader population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ai. `Dataset.Tabular`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the ways practitioners can use this 2D structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||\n",
    "|---|---|---|\n",
    "Multiple subjects (1 row per sample) | * | Multi-variate 1D (1 col per attribute)\n",
    "Single subject (1 row per timestamp) | * | Multi-variate 1D (1 col per attribute)\n",
    "Multiple subjects (1 row per timestamp) | * | Uni-variate 0D (1 col per sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tabular datasets may contain both features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "└── `Dataset.Tabular.from_df()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "dataset = Dataset.Tabular.from_df(\n",
    "    dataframe\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**df** | DataFrame | Required | [pd.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas-dataframe) with int-based single index. DataFrames are always ingested.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration)\n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration)\n",
    "**description** | str | None | See [Registration](#1a.-Registration)\n",
    "**name** | str | None | See [Registration](#1a.-Registration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.Tabular.from_path()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Tabular.from_path(\n",
    "    file_path\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , header\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**file_path** | str | Required | Parsed based on how the file name ends (.parquet, .tsv, .csv)\n",
    "**ingest** | bool | True | See [Registration](#1a.-Registration). Defaults to True because I don't want to rely on CSV files as a source of truth for dtypes, and compression works great in Parquet.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration)\n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration)\n",
    "**header** | object | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration)\n",
    "**name** | str | None | See [Registration](#1a.-Registration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1aii. `Dataset.Sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the ways practitioners can use this 3D structure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||\n",
    "|---|---|---|\n",
    "Single subject (1 patient) | * | Multiple 2D sequences\n",
    "Multiple subjects | * | Single 2D sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sequence datasets are somewhat multi-modal in that, in order to perform supervised learning on them, they must eventually be paired with a `Dataset.Tabular` that acts as its `Label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.Sequence.from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Sequence.from_numpy(\n",
    "    arr3D_or_npyPath\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype        \n",
    "    , description\n",
    "    , name           \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**arr3D_or_npyPath** | object / str | Required | 3D array in the form of either an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) or [npy](https://numpy.org/doc/stable/reference/generated/numpy.save.html) file path\n",
    "**ingest** | bool | None | See [Registration](#1a.-Registration). If left blank, ndarrays will be ingested and npy will not. Errors if ndarray and False.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration)\n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration)\n",
    "**description** | str | None | See [Registration](#1a.-Registration)\n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1aiii. `Dataset.Image`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the ways you can practitioners this 4D structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||\n",
    "|---|---|---|\n",
    "Single subject (1 patient) | * | Multiple 3D images\n",
    "Multiple subjects | * | Single 3D image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can ingest 4D data using either:\n",
    "- [The Pillow library, which supports various formats](pillow.readthedocs.io/en/stable/handbook/image-file-formats.html)\n",
    "- Or NumPy arrays as a simple alternative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Image datasets are somewhat multi-modal in that, in order to perform supervised learning on them, they must eventually be paired with a `Dataset.Tabular` that acts as its `Label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.Image.from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Image.from_numpy(\n",
    "    arr4D_or_npyPath\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype        \n",
    "    , description\n",
    "    , name       \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**arr4D_or_npyPath** | object / str | Required | 4D array in the form of either an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) or [npy](https://numpy.org/doc/stable/reference/generated/numpy.save.html) file path\n",
    "**ingest** | bool | None | See [Registration](#1a.-Registration). If left blank, ndarrays will be ingested and npy will not. Errors if ndarray and False.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration) \n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration) \n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.Image.from_folder()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Image.from_folder(\n",
    "    folder_path\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**folder_path** | str | Required | Folder of images to be ingested via Pillow. All images must be cropped to the same dimensions ahead of time.\n",
    "**ingest** | bool | False | See [Registration](#1a.-Registration) \n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration) \n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration) \n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.Image.from_urls()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Image.from_urls(\n",
    "    urls\n",
    "    , source_path\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**urls** | list(str) | Required | URLs that point to an image to be ingested via Pillow. All images must be cropped to the same dimensions ahead of time.\n",
    "**source_path** | str  | None | Optionally record a shared directory, bucket, or FTP site where images are stored. The backend won't use this information for anything. |\n",
    "**ingest** | bool | False | See [Registration](#1a.-Registration)\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration) \n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration) \n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are exposed to end-users in case they want to inspect the data that they have ingested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.to_arr()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**columns** | list(str) | None | If left blank, includes all columns\n",
    "**samples** | list(int) | None | If left blank, includes all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Subclass | Returns\n",
    "--- | ---\n",
    "Tabular | ndarray.ndim==2\n",
    "Sequence | ndarray.ndim==3\n",
    "Image | ndarray.ndim==4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.to_df()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**columns** | list(str) | None | If left blank, includes all columns\n",
    "**samples** | list(int) | None | If left blank, includes all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Subclass | Returns\n",
    "--- | ---\n",
    "Tabular | DataFrame\n",
    "Sequence | list(DataFrame)\n",
    "Image | list(list(DataFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.to_pillow()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**samples** | list(int) | None | If left blank, includes all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subclass | Returns\n",
    "--- | ---\n",
    "Image | list(PIL.Image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Dataset.get_dtypes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**columns** | list(str) | None | If left blank, includes all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how the initial `Dataset.dtype` was formatted [e.g. single np.type / str(np.type) / dict(column=np.type)], this function intentionally returns then dtype of each column within a `dict(column=str(np.type)` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields in the Dataset table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**typ** | CharField | The Dataset type: Tabular, Sequence, Image\n",
    "**source_format** | CharField | The file format (Parquet, CSV, TSV)  or in-memory class (DataFrame, ndarray)\n",
    "**source_path** | CharField | The path of the original file/ folder\n",
    "**urls** | JSONField | A list of URLs as an alternative to file paths/ folders\n",
    "**columns** | JSONField | List of str-based names for each column\n",
    "**dtypes** | JSONField | The type of each column. Tabular dtype is saved in `dict(column=str(np.type))` format. Where Sequence and Image dtype is saved in a singular `str(np.type)`\n",
    "**shape** | JSONField | Human-readable dictionary about the dimensions of the data e.g. `samples:10, columns:5`\n",
    "**sha256_hexdigest** | CharField | A hash of the data to determine its uniqueness for versioning.\n",
    "**memory_MB** | IntegerField | Size of the dataset in megabytes when loaded into memory\n",
    "**contains_nan** | BooleanField | Whether or not the dataset contains any blank cells\n",
    "**header** | PickleField | `pd.read_csv(header)` for TSV/CSV files.\n",
    "**is_ingested** | BooleanField | Quick flag to see if the data was ingested. Exists to prevent querying the `blob` field unnecessarily. \n",
    "**blob** | BlobField | The raw bytes of the data obtained via `BytesIO().getvalue`\n",
    "**version** | IntegerField | The auto-incrementing version number assigned to unique datasets that share name\n",
    "**description** | CharField | What information does this dataset contain? What is unique about this dataset/ version -- did you edit the raw data, add rows, or change column names/ dtypes?\n",
    "**name** | CharField | Triggers dataset *versioning*. Datasets that share a name will be assigned an auto-incrementing `version:int` number provided that they are not duplicates of each other based on a `sha256_hexdigest:str` hash. If you try to create an exact duplicate, it will warn you and `return` the matching duplicate instead of creating a new entity. This behavior makes it easy to rerun pipelines where Datasets are created inline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determines the columns that will be used as predictive features during training. Columns is always the last dimension `shape[-1]` of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Feature.from_dataset()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Feature.from_dataset(\n",
    "    dataset_id\n",
    "    , include_columns\n",
    "    , exclude_columns\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**dataset_id** | int | Required | `Dataset.id` from which you want to derive `Dataset.columns`.\n",
    "**include_columns**  | list(str) | None | Specify columns that *will* be included in the Feature. All columns that are not specified will *not* be included.\n",
    "**exclude_columns** | list(str) | None | Specify columns that will *not* be included in the Feature. All columns that are not specified *will* be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If neither `include_columns` nor `exclude_columns` is defined, then all columns will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Both `include_columns` and `exclude_columns` cannot be used at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theses methods wrap Dataset's [fetch](#1b.-Fetch) methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method | Arguments | Returns\n",
    "--- | --- | ---\n",
    "**to_arr()** | columns:list(str)=Feature.columns, samples:list(int)=None | ndarray 2D / 3D / 4D\n",
    "**to_df()** | columns:list(str)=Feature.columns, samples:list(int)=None | df / list(df) / list(list(df))\n",
    "**get_dtypes()** | columns:list(str)=Feature.columns | dict(column=str(np.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields in the Feature table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**columns** | JSONField | The columns included in this featureset\n",
    "**columns_excluded** | JSONField | The columns, if any, in the dataset that were not included\n",
    "**fitted_featurecoders** | PickleField | When `FeatureCoder`'s `fit` an sklearn preprocessor to these columns, the fit objects are saved here for downstream `inverse_transform`'ing\n",
    "**dataset** | ForeignKeyField | Where these columns came from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determines the column(s) that will be used as a target during supervised analysis. Do no create a Label if you intend to conduct unsupervised/ self-supervised analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Label.from_dataset()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Label.from_dataset(\n",
    "    dataset_id\n",
    "    , columns\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**dataset_id** | int | Required | `Dataset.id` from which you want to derive `Dataset.columns`. Only Tabular Datasets may be used as a Label.\n",
    "**columns**  | list(str) | None | Specify columns that *will* be included in the Label. If left blank, defaults to all columns. If more than 1 column is provided, then the data in those columns must be in One-Hot Encoded (OHE) format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theses methods wrap Dataset's [fetch](#1b.-Fetch) methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method | Arguments | Returns\n",
    "--- | --- | ---\n",
    "**to_arr()** | columns:list(str)=Label.columns, samples:list(int)=None | ndarray 2D / 3D / 4D\n",
    "**to_df()** | columns:list(str)=Label.columns, samples:list(int)=None | df / list(df) / list(list(df))\n",
    "**get_dtypes()** | columns:list(str)=Label.columns | dict(column=str(np.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields in the Feature table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**columns** | JSONField | The column(s) included in this featureset. \n",
    "**column_count** | IntegerField | The number of columns in the Label. Used to determine if it is in validated OHE format or not\n",
    "**unique_classes** | JSONField | Records all of the different values found in categorical columns. Not used for continuous columns.\n",
    "**fitted_labelcoder** | PickleField | When a `LabelCoder` `fit`'s an sklearn preprocessor to these columns, the fit objects are saved here for downstream `inverse_transform`'ing\n",
    "**dataset** | ForeignKeyField | Where these columns came from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have time series data then you do not need interpolation.\n",
    "\n",
    "If you have continuous columns with missing data in a time series, then interpolation allows you to fill in those blanks mathematically. It does so by fitting a curve to each column. Therefore each column passed to an interpolater must satisfy: `np.issubdtype(dtype, np.floating)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation is the first preprocessor because you need to fill in blanks prior to encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `pandas.DataFrame.interpolate`\n",
    "> \n",
    "> https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html\n",
    "> \n",
    "> Is utilized due to its ease of use, variety of methods, and **support of sparse indices**. However, it does not follow the `fit/transform` pattern like many of the class-based sklearn preprocessors, so the interpolated training data is concatenated with the evalaution split during the interpolation of evaluation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the default settings if `interpolate_kwargs=None` that get passed to `df.interpolate()`. In my experience, `method=spline` produces the best results. However, if either (a) spline fails to fit to your data, or (b) you know that your pattern is linear - then try `method=linear`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "interpolate_kwargs = dict(\n",
    "    method            = 'spline'\n",
    "    , limit_direction = 'both'\n",
    "    , limit_area      = None\n",
    "    , axis            = 0\n",
    "    , order           = 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the sample dimension is different for each Dataset Type, they approach interpolation differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Type | Approach\n",
    "--- | ---\n",
    "**Tabular** | Unlike encoders, there is no `fit` object. So first the training data rows are interpolated independently. Then, when it comes time to interpolate other splits like validation, the training data is included in the sequence to be interpolated.\n",
    "**Sequence** | Interpolation is ran on each 2D sequence separately\n",
    "**Image** | Interpolation is ran on each 2D channel separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. `LabelInterpolater`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label is intended for a single column, so only 1 Interpolater will be used during `Label.preprocess()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4ai. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `LabelInterpolater.from_label()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "LabelInterpolater.from_label(\n",
    "    label_id\n",
    "    , process_separately\n",
    "    , interpolate_kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**label_id** | int | Required | Points to the `Label.columns` to use\n",
    "**process_separately** | bool | True | Used to restrict the fit to the training data, this may be flipped to `False`. However, doing so causes data leakage.\n",
    "**interpolate_kwargs** | dict | None | Gets passed to `df.interpolate()`. See [Interpolate](#4.-Interpolate) section for defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4aii. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields in the LabelInterpolater table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**process_separately** | BooleanField | Whether or not the training data was interpolated by fitting to the entire dataset or not. Indicator of data leakage.\n",
    "**interpolate_kwargs** | JSONField | Gets passed to `df.interpolate()`. See [Interpolate](#4.-Interpolate) section for defaults.\n",
    "**matching_columns** | JSONField | The columns that were successfully interpolated\n",
    "**label** | ForeignKeyField | The Label that this LabelInterpolater is applied to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. `FeatureInterpolater`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *multivariate* datasets, columns/dtypes may need to be handled differently. So we use column/dtype filters to apply separate transformations. If the first transformation's filter includes a certain column/dtype, then subsequent filters may not include that column/dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4bi. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `FeatureInterpolater.from_feature()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "FeatureInterpolater.from_feature(\n",
    "    feature_id\n",
    "    , process_separately\n",
    "    , interpolate_kwargs\n",
    "    , dtypes\n",
    "    , columns\n",
    "    , verbose\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**feature_id** | int | Required | Points to the `Feature.columns` to use\n",
    "**process_separately** | bool | True | Used to restrict the fit to the training data, this may be flipped to `False`. However, doing so causes data leakage.\n",
    "**interpolate_kwargs** | dict | None | The `interpolate_kwargs:dict=None` object is what gets passed to Pandas interpolation. In my experience, `method=spline` produces the best results. However, if either (a) spline fails to fit to your data, or (b) you know that your pattern is linear - then try `method=linear`.\n",
    "**dtypes** | list(str) | None | The dtypes to include\n",
    "**columns** | list(str) | None | The columns to include. Errors if any of the columns were already included by dtypes.\n",
    "**verbose** | bool | True | If True, messages will be printed about the status of the interpolaters as they attempt to fit on the filtered columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4bii. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields in the FeatureInterpolater table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**index** | IntegerField | Zero-based auto-incrementer that counts the number of FeatureInterpolaters attached to a Feature.\n",
    "**process_separately** | BooleanField | Whether or not the training data was interpolated by fitting to the entire dataset or not. Indicator of data leakage.\n",
    "**interpolate_kwargs** | JSONField | Gets passed to `df.interpolate()`. See [Interpolate](#4.-Interpolate) section for defaults.\n",
    "**matching_columns** | JSONField | The columns that matched the filter\n",
    "**leftover_columns** | JSONField | The columns that were not included in the filter\n",
    "**leftover_dtypes** | JSONField | The dtypes that were not included in the filter\n",
    "**original_filter** | JSONField | `dict().keys()==['include','dtypes','columns']`\n",
    "**feature** | ForeignKeyField | The Feature that this FeatureInterpolater is applied to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data into numerical format that is close to zero. Reference [Encoding](https://aiqc.readthedocs.io/en/latest/pages/explainer.html) for more information.\n",
    "\n",
    "There are two phases of encoding:\n",
    "1. `fit` on train - where the encoder learns about the values of the samples made available to it. Ideally, you only want to `fit` aka learn from your training split so that you are not [leaking](https://towardsdatascience.com/data-leakage-5dfc2e0127d4) information from your validation and test spits into your model! However, categorical encoders are always fit on the entire dataset because they are not prone to leakage and any weights tied to empty OHE inputs will zero-out.\n",
    "2. `transform` each split/fold\n",
    "\n",
    "\n",
    "> Only [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) methods are officially supported, but we have experimented with `sklearn.feature_extraction.text.CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. `LabelCoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label is intended for a single column, so only 1 LabelCoder will be used during `Label.preprocess()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Unfortunately, the name \"LabelEncoder\" is occupied by `sklearn.preprocessing.LabelEncoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5ai. Create\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `LabelCoder.from_label()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "LabelCoder.from_label(\n",
    "    label_id\n",
    "    , sklearn_preprocess\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**label_id** | int | Required | Points to the `Label.columns` to use\n",
    "**sklearn_preprocess** | object | Required | An instantiated `sklearn.preprocessing` class-based encoder - e.g. `StandardScaler()` neither `StandardScaler` nor  `scale()`. AIQC will automatically correct the attributes of your encoder to smooth out any common errors they would cause. For example, preventing sparse SciPy matrix output (errors during tensor conversion) and data `copy()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5aii. Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields in the LabelCoder table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**only_fit_train** | BooleanField | Whether or not the encoder was fit on the training data or the entire dataset\n",
    "**is_categorical** | BooleanField | If the encoder is meant for categorical data, and therefore automatically fit on the entire dataset\n",
    "**sklearn_preprocess** | PickleField | The instantiated sklearn.preprocessing class that was fit\n",
    "**matching_columns** | JSONField | The columns that matched the dtype/ column name filters\n",
    "**encoding_dimension** | CharField | Did the encoder succeed on 1D/ 2D uni-column/ 2D multi-column?\n",
    "**label** | ForeignKeyField | The Label that this LabelCoder is applied to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. `FeatureCoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *multivariate* datasets, columns/dtypes may need to be handled differently. So we use column/dtype filters to apply separate transformations. If the first transformation's filter includes a certain column/dtype, then subsequent filters may not include that column/dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5bi. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `FeatureCoder.from_feature()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "FeatureCoder.from_feature(\n",
    "    feature_id\n",
    "    , sklearn_preprocess\n",
    "    , include\n",
    "    , dtypes\n",
    "    , columns\n",
    "    , verbose\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**feature_id** | int | Required | Points to the `Feature.columns` to use\n",
    "**sklearn_preprocess** | object | Required | An instantiated `sklearn.preprocessing` class-based encoder - e.g. `StandardScaler()` neither `StandardScaler` nor  `scale()`. AIQC will automatically correct the attributes of your encoder to smooth out any common errors they would cause. For example, preventing sparse SciPy matrix output (errors during tensor conversion) and data `copy()`.\n",
    "**include** | bool | True | Whether to include or exclude the dtypes/columns that match the filter. You can create a filter for all columns by setting `include=False` and then setting both `dtypes` and `columns` to `None`.\n",
    "**dtypes** | list(str) | None | The dtypes to filter\n",
    "**columns** | list(str) | None | The columns to filter. Errors if any of the columns were already used by dtypes.\n",
    "**verbose** | bool | True | If True, messages will be printed about the status of the encoders as they attempt to fit on the filtered columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5bii. Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields in the FeatureCoder table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**index** | IntegerField | Zero-based auto-incrementer that counts the number of FeatureCoders attached to a Feature.\n",
    "**sklearn_preprocess** | PickleField | The instantiated sklearn.preprocessing class that was fit\n",
    "**encoded_column_names** | JSONField | After the columns are encoded, what are their names? OHE appends `_<category>` to the original column names as it expands\n",
    "**matching_columns** | JSONField | The columns that matched the filter\n",
    "**leftover_columns** | JSONField | The columns that were not included in the filter\n",
    "**leftover_dtypes** | JSONField | The dtypes that were not included in the filter\n",
    "**original_filter** | JSONField | `dict().keys()==['include','dtypes','columns']`\n",
    "**encoding_dimension** | CharField | Did the encoder succeed on 1D/ 2D uni-column/ 2D multi-column?\n",
    "**only_fit_train** | BooleanField | Whether or not the encoder was fit on the training data or the entire dataset\n",
    "**is_categorical** | BooleanField | If the encoder is meant for categorical data, and therefore automatically fit on the entire dataset\n",
    "**feature** | ForeignKeyField | The Feature that this FeatureCoder is applied to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes the shape of data. Only supports Features, not Labels.\n",
    "\n",
    "Reshaping is applied at the end of `Feature.preprocess()`. So if the feature data has been altered via time series windowing or One Hot Encoder, then those changes will be reflected in the shape that is fed to `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with architectures that are highly dimensional such convolutional and recurrent networks (Conv1D, Conv2D, Conv3D / ConvLSTM1D, ConvLSTM2D, ConvLSTM3D), you'll often find yourself needing to reshape data to fit a layer's required input shape. \n",
    "\n",
    "- *Reducing unused dimensions* - When working with grayscale images (1 channel, 25 rows, 25 columns) it's better to use Conv1D instead of Conv2D.\n",
    "- *Adding wrapper dimensions* - Perhaps your data is a fit for ConvLSTM1D, but that layer is only supported in the nightly TensorFlow build so you want to add a wrapper dimension in order to use the production-ready ConvLSTM2D.\n",
    "\n",
    "AIQC favors a *\"channels_first\"* (samples, channels, rows, columns) approach as opposed to *\"channels_last\"* (samples, rows, columns, channels).\n",
    "\n",
    "> *Can't I just reshape the tensors during the training loop?* You could. However, AIQC systemtically provides the shape of features and labels to `Algorith.fn_build` to make designing the topology easier, so it's best to get the shape right beforehand. Additionally, if you reshape your data within the training loop, then you may also need to reshape the output of `Algorithm.fn_predict` so that it is correctly formatted for automatic post-processing. It's also more computationally efficient to do the reshaping once up front."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reshape_indices` argument is ultimately fed to [np.reshape(newshape)](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html). We use *index n* to point to the value at `ndarray.shape[n]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshaping by Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a 4D feature consisting of 3D images `(samples * channels * rows * columns)`. Our problems is that the images are B&W, so we don't want a color channel because it would add unecessary dimensionality to our model. Thus, we want to drop the dimension at the shape index `1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reshape_indices = (0,2,3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have wrangled ourselves a 3D feature consisting of 2D images `(samples * rows * columns)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshaping Explicitly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if the dimensions we want cannot be expressed by rearranging the existing indices? If you define a number as a `str`, then that number will be used as directly as the value at that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if I wanted to add an extra wrapper dimension to my data to serve as a single color channel, I would simply do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reshape_indices = (0,'1',1,2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Then couldn't I just hardcode my shapes with strings?* Yes, but `FeatureShaper` is applied to all of the splits, which are assumed to have different shapes, which is why we use the indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplicative Reshaping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need to stack/nest dimensions. This requires multiplying one shape index by another. \n",
    "\n",
    "For example, if I have a 3 separate hours worth of data and I want to treat it as 180 minutes, then I need to go from a shape of (3 hours * 60 minutes) to (180 minutes). Just provide the shape indices that you want to multiply in a `tuple` like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <!> if your model is unsupervised (aka generative or self-supervised), then it must output data in *\"column (aka width) last\"* shape. Otherwise, automated column decoding will be applied along the wrong dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `FeatureShaper.from_feature()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "FeatureShaper.from_feature(\n",
    "    feature_id\n",
    "    , reshape_indices\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**feature_id** | int | Required | The `Feature.id` to use\n",
    "**reshape_indices** | tuple(int/str/tuple) | Required | See [Strategies](#6a.-Strategies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "These are the fields of the FeatureShaper table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**reshape_indices** | PickleField | See [#Reshaping-by-Index](#6b.-Create).Pickle because tuple has no JSON equivalent. \n",
    "**column_position** | IntegerField | The shape index used for columns aka width.\n",
    "**feature** | ForeignKeyField | The Feature that reshaping is applied to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text\n",
    "**item** | type | text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window facilitates sliding windows for a time series Feature. It does not apply to Labels. This is used for unsupervised (aka self-supervised) walk-forward forecasting for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`size_window` determine how many timepoints are included in a window, and `size_shift` determines how many timepoints to slide over before defining a new window. \n",
    "\n",
    "> For example, if we want to be able to *predict the next 7 days* worth of weather *using the past 21 days* of weather, then our `size_window=21` and our `size_shift=7`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with stratified windowed data demands a systematic approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window_dimensions](../_static/images/api/window_dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Windowing always increases dimensionality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data is windowed, its dimensionality increases by 1. Why? Well, originally we had a *single time series*. However, if we window that data, then we have *many time series subsets*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![interpolate_windows](../_static/images/api/interpolate_windows.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the highest dimension, it becomes the \"sample\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what dimensionality the original data has, it will be windowed along the first dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the windows now serve as the samples, which is important for stratification. If we have a year's worth of windows, we don't want all of our training windows to come from the same season. Therefore, Window must be created prior to Splitset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Windowing may causes overlap in splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to increasing the dimensionality of our data, it makes it harder to nail down the boundaries of our splits in order to prevent data leakage.\n",
    "\n",
    "As seen in the diagram above, the timesteps of the train and test splits may *overlap*. So if we are fitting an interpolater to our training split, the first 3 NaNs would be included, but the last 2 would not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Windows](../_static/images/api/sliding_windows.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shifted and unshifted windows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a walk-forward analysis, we learn about the future by looking at the past. So we need 2 sets of windows:\n",
    "\n",
    "- *Unshifted windows* (orange in diagram above): represent the past and serves as the features we learn from\n",
    "- *Shifted windows* (green in diagram above): represent the future and serves as the target we predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when conducting inference, we are trying to predict the shifted windows not learn from them. So we don't need to record any shifted windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Window.from_feature()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Window.from_feature(\n",
    "    feature_id\n",
    "    , size_window\n",
    "    , size_shift\n",
    "    , record_shifted\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**dataset_id** | int | Required | `Feature.id` from which you want to derive windows.\n",
    "**size_window**  | int | Required | The number of timesteps to include in a window.\n",
    "**size_shift**  | int | Required | The number of timesteps to shift forward.\n",
    "**record_shifted**  | bool | True | Whether or not we want to keep a shifted set of windows around. During pure inference, this is False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "These are the fields of the Window table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**size_window** | IntegerField | Number of timesteps in each window\n",
    "**size_shift** | IntegerField | The number of timesteps in the shift forward.\n",
    "**window_count** | IntegerField | Not a relationship count! Number of windows in the dataset. This becomes the new samples dimension for stratification. \n",
    "**samples_unshifted** | JSONField | Underlying sample indices of each window in the past-shifted windows.\n",
    "**samples_shifted** | JSONField | Underlying sample indices of each window in the future-shifted windows.\n",
    "**feature** | ForeignKeyField | The Feature that this windowing is applied to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Splitset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for sample stratification. Reference [Stratification](https://aiqc.readthedocs.io/en/latest/pages/explainer.html) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split | Description\n",
    "--- | ---\n",
    "**train** | The samples that the model will be trained upon. Later, we’ll see how we can make *cross-folds from our training split*. Unsupervised learning will only have a training split.\n",
    "**validation** (optional) | The samples used for training evaluation. Ensures that the test set is not revealed to the model during training.\n",
    "**test** (optional) | The samples the model has never seen during training. Used to assess how well the model will perform on unobserved, natural data when it is applied in the real world aka how generalizable it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because Splitset groups together all of the data wrangling entities (Features, Label, Folds) it essentially represents a *Pipeline*, which is why it bears the name Pipeline in the High-Level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Splitset.make()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Splitset.make(\n",
    "    feature_ids\n",
    "    , label_id\n",
    "    , size_test\n",
    "    , size_validation\n",
    "    , bin_count\n",
    "    , fold_count\n",
    "    , unsupervised_stratify_col\n",
    "    , name\n",
    "    , description\n",
    "    , predictor_id\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**feature_ids** | list(int) | Required | Multiple `Feature.id`'s may be included to enable multi-modal (aka mixed data-type) analysis. All of these Features must have the same number of samples.\n",
    "**label_id** | int | None | The Label to be used as a target for supervised analysis. Must have the sample number of samples as the Features.\n",
    "**size_test** | float | None | Percent of samples to be placed into the test split. Must be `> 0.0` and `< 1.0`.\n",
    "**size_validation** | float | None | Percent of samples to be placed into the validation split. Must be `> 0.0` and `< 1.0`. If this is not None and used in combination with `fold_count`, then there will be 4 splits.\n",
    "**bin_count** | int | None | For continous stratification columns, how many bins (aka quantiles) should be used?\n",
    "**fold_count** | int | None | The number or cross-validation folds to generate. See [Cross-Validation](#5b.-Cross-Validation).\n",
    "**unsupervised_stratify_col** | str | None | Used during unsupervised analysis. Specify a column from the first Feature in feature_ids to use for stratification. For example, when forecasting, it may make sense to stratify by the day of the year.\n",
    "**name** | str | None | Used for versioning a pipeline (collection of inputs, label, and stratification). Two versions cannot have identical attributes.\n",
    "**description** | str | None | What is unique about this this pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `size_train = 1.00 - (size_test + size_validation)` the backend ensures that the sizes sum to 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *How does continuous binning work?* Reference the handy `Pandas.qcut()`  and the source code `pd.qcut(x=array_to_bin, q=bin_count, labels=False, duplicates='drop')` for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is triggered by `fold_count:int` during Splitset creation. Reference the [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html) to learn more about cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross fold objects](../_static/images/api/cross_fold_objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the diagram above is a `Fold` object.\n",
    "\n",
    "Each green/blue box represents a bin of stratified samples. During preprocessing and training, we rotate which blue bin serves as the validation samples (`fold_validation`). The remaining green bins in the row serve as the training samples (`folds_train_combined`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we defined `fold_count=5`. What are the implications?\n",
    "- Creates 5 `Folds` related to a `Splitset`.\n",
    "- 5x more models will be trained for each experiment.\n",
    "- 5x more preprocessing and caching; the backend must preprocess each Fold separately to prevent data leakage by excluding `fold_validation` from the `fit`. Fits are saved to the Fold object as opposed to the Splitset object.\n",
    "- 5x more evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DO NOT use cross-validation unless your *(total sample count / fold_count)* still gives you an accurate representation of your entire sample population. If you are ignoring that advice and stretching to perform cross-validation, then at least ensure that *(total sample count / fold_count)* is evenly divisible. Folds naturally have fewer samples, so a handful of incorrect predictions have the potential to offset your aggregate metrics. Both of these tips help avoid poorly stratified/ undersized folds that seem to perform either too well (only most common label class present) or poorly (handful of samples and a few inaccurate prediction on an otherwise good model).\n",
    "> \n",
    "> Candidly, if you've ever performed cross-validation manually, let alone systematically, you'll know that, barring stratification of continuous labels, it's easy enough to construct the folds, but then it's a pain to generate performance metrics (e.g. `zero_division`, absent OHE classes) due to the absence of outlying classes and bins.  Time has been invested to handle these scenarios elegantly so that folds can be treated as first-class-citizens alongside splits. That being said, if you try to do something undersized like \"150 samples in their dataset and a `fold_count` > 3 with `unique_classes` > 4,\" then you may run into edge cases.\n",
    ">\n",
    "> Cross validation is only included in AIQC to allow best practices to be upheld and to show off the power of systematic preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields of the Splitset table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**cache_path** | CharField | Where the splitset stores its cached samples\n",
    "**cache_hot** | BooleanField | If the samples are currently stored in the cache\n",
    "**samples** | JSONField | The bins that splits have been stratified into `dict(split=[sample_indices])`\n",
    "**sizes** | JSONField | Human-readable sizes of the splits `dict(split=dict(percent=float,count=int))`\n",
    "**supervision** | CharField | Logical flag indicating if this Splitset has a Label.\n",
    "**has_validation** | BooleanField | Logical flag indicating if this Splitset has a validation split.\n",
    "**fold_count** | IntegerField | The number of cross-validation `Folds` that belong to this Splitset\n",
    "**bin_count** | IntegerField | The number of bins used to stratify a continuous column label or unsupervised_stratify column\n",
    "**unsupervised_stratifyCol** | CharField | Used during unsupervised analysis. Specify a column from the first Feature in feature_ids to use for stratification. For example, when forecasting, it may make sense to stratify by the day of the year.\n",
    "**key_train** | CharField | `'train'` by default, but `'folds_train_combined'` if Splitset has Folds. `None` for an inference splitset.\n",
    "**key_evaluation** | CharField | `'test'` if neither validation split nor Folds are used. `'validation'` if a validation split is used. `'fold_validation'` if Splitset has Folds. `None` for an inference splitset.\n",
    "**key_test** | CharField | `'test'` by default. `None` for an inference splitset.\n",
    "**version** | IntegerField | [TBD]\n",
    "**label** | ForeignKeyField | The Label, if any, that supervises this splitset\n",
    "**predictor** | DeferredForeignKey | During inference, a new Splitset of samples to be predicted may attach to a Predictor. Samples dict will bear the key of the `Predictor.Splitset.count()` e.g. `'infer_0'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been prepared, we transition to the 2nd half of the ORM where the focus is the logic that will be applied to that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Algorithm contains all of the components needed to construct, train, and use our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the [tutorials](../pages/gallery.html) for examples of how Algorithms are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble an architecture consisting of components defined in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `**hp` kwargs are common to every Algorithm function except `fn_predict`. They are used to systematically pass a dictionary of *hyperparameters* into these functions. See Hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Algorithm.make()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Algorithm.make(\n",
    "    library\n",
    "    , analysis_type\n",
    "    , fn_build\n",
    "    , fn_train\n",
    "    , fn_predict\n",
    "    , fn_lose\n",
    "    , fn_optimize\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**library** | str | Required | 'keras' or 'pytorch' depending on the type of model defined in `fn_build`\n",
    "**analysis_type** | str | Required | 'classification_binary', 'classification_multi', or 'regression'. Unsupervised/ self-supervised falls under regression. Used to determine which performance metrics are run. Errors if it is incompatible with the Label provided: e.g. classification_binary is incompatible with an np.floating Label.column.\n",
    "**fn_build** | func | Required | See below. Build the model architecture.\n",
    "**fn_train** | func | Required | See below. Train the model.\n",
    "**fn_predict** | func | None | See below. Run the model.\n",
    "**fn_lose** | func | None | See below. Calculate loss.\n",
    "**fn_optimize** | func | None | See below. Optimization strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def fn_build(\n",
    "    features_shape:tuple\n",
    "    , label_shape:tuple\n",
    "    , **hp:dict\n",
    "):\n",
    "    # Define tf/torch model\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `*_shape` arguments contain the shape of a single sample, as opposed to a batch or entire dataset. `features_shape` is plural because it may contain the shape of multiple features. However, if only 1 feature was used then it will not be inside a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def fn_train(\n",
    "    model:object\n",
    "    , loser:object\n",
    "    , optimizer:object\n",
    "    , train_features:ndarray\n",
    "    , train_label:ndarray\n",
    "    , eval_features:ndarray\n",
    "    , eval_label:ndarray\n",
    "    , **hp:dict\n",
    "):\n",
    "    # Define training/ eval loop. \n",
    "    # See `utils.pytorch.fit`\n",
    "    \n",
    "    # if tensorflow\n",
    "    return model \n",
    "    # if torch\n",
    "    # See `utils.pytorch.fit` and history metrics below\n",
    "    return history:dict, model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Where are the defaults for optional functions defined?* See [utils.tensorflow](https://github.com/aiqc/AIQC/blob/main/aiqc/utils/tensorflow.py) and [utils.pytorch](https://github.com/aiqc/AIQC/blob/main/aiqc/utils/pytorch.py) for examples of loss, optimization, and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def fn_predict(model:object, features:ndarray):\n",
    "    #if classify. predictions always ordinal, never OHE.\n",
    "    return prediction, probabilities #both as ndarray\n",
    "\n",
    "    #if regression\n",
    "    return prediction #ndarray\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def fn_lose(**hp:dict):\n",
    "    # Define tf/torch loss function\n",
    "    return loser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def fn_optimize(**hp:dict):\n",
    "    # Define tf/torch optimizer \n",
    "    return optimizer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Algorithm.get_code()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reveals the strings of the Algorithm functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dict(\n",
    "    fn_build      = utils.dill.reveal_code(Algorithm.fn_build)\n",
    "    , fn_lose     = utils.dill.reveal_code(Algorithm.fn_lose)\n",
    "    , fn_optimize = utils.dill.reveal_code(Algorithm.fn_optimize)\n",
    "    , fn_train    = utils.dill.reveal_code(Algorithm.fn_train)\n",
    "    , fn_predict  = utils.dill.reveal_code(Algorithm.fn_predict)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. PyTorch `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides an abstraction that eliminates the boilerplate code normally required to train and evaluate a PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before training - it shuffles samples, batches samples, and then shuffles batches.\n",
    "- During training - it calculates batch loss, epoch loss, and epoch history metrics.\n",
    "- After training - it calculates metrics for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model, history = utils.pytorch.fit(\n",
    "    # These arguments come directly from `fn_train`\n",
    "    model\n",
    "    , loser\n",
    "    , optimizer\n",
    "    \n",
    "    , train_features\n",
    "    , train_label\n",
    "    , eval_features\n",
    "    , eval_label\n",
    "    \n",
    "    # These arguments are user-defined\n",
    "    , epochs\n",
    "    , batch_size\n",
    "    , enforce_sameSize\n",
    "    , allow_singleSample\n",
    "    , metrics\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-Defined Arguments | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**epochs** | int | 30 | The number of times to loop over the features\n",
    "**batch_size** | int | 5 | Divides features and lables into chunks to be trained upon\n",
    "**enforce_sameSize** | bool | True | If `True`, drops `len(batch!=batch_size)`\n",
    "**allow_singleSample** | bool | False | If `False`, drops `len(batch!=1)`\n",
    "**metrics** | list(torchmetrics.metric()) | None | List of instantiated `torchmetrics` classes e.g. `Accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. History Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the `Predictor.history` object is to record the training and evaluation metrics at the end of each epic so that they can be interpretted in the learning curve plots. Reference the [visualization](visualization.html) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Keras*: any `metrics=[]` specified are automatically added to the `History` callback object.\n",
    "\n",
    "- *PyTorch*: if you use `fit` seen above, then you don't need to worry about this. Users are responsible for calculating their own metrics (we recommend the `torchmetrics` package) and placing them into a `history` dictionary that mirrors the schema of the Keras history object. Reference the torch [examples](gallery/pytorch/multi_class.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The schema of the `history` dictionary is as follows: `dict(*:ndarray, val_*=ndarray)`. For example, if you wanted to record the history of the 'loss' and 'accuracy' metrics manually for PyTorch, you would construct it like so:\n",
    "\n",
    "```python\n",
    "history = dict(\n",
    "    loss           = ndarray\n",
    "    , val_loss     = ndarray\n",
    "    \n",
    "    , accuracy     = ndarray\n",
    "    , val_accuracy = ndarray\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9c. TensorFlow Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Early stopping* isn't just about efficiency in reducing the number of `epochs`. If you've specified 300 epochs, there's a chance your model catches on to the underlying patterns early, say around 75-125 epochs. At this point, there's also good chance what it learns in the remaining epochs will cause it to overfit on patterns that are specific to the training data, and thereby and lose it's simplicity/ generalizability.\n",
    "\n",
    "> The `metric=val_*` prefix refers to the evaluation samples.\n",
    ">\n",
    "> Remember, regression does not have accuracy metrics.\n",
    ">\n",
    "> `TrainingCallback.MetricCutoff` is a custom class we wrote to make *early stopping* easier, so you won't find information about it in the official Keras documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placed within `fn_train`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    from aiqc.utils.tensorflow import TrainingCallback\n",
    "    \n",
    "    #Define one or more metrics to monitor.\n",
    "    metrics_cuttoffs = [\n",
    "        dict(metric='val_accuracy', cutoff=0.96, above_or_below='above'),\n",
    "        dict(metric='val_loss', cutoff=0.1, above_or_below='below')\n",
    "    ]\n",
    "    cutoffs = TrainingCallback.MetricCutoff(metrics_cuttoffs)\n",
    "\n",
    "    # Pass it into keras callbacks\n",
    "    model.fit(\n",
    "        # other fit args\n",
    "        callbacks = [cutoffs]\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9d. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields of the Splitset table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type\n",
    "--- | ---\n",
    "**library** | CharField\n",
    "**analysis_type** | CharField\n",
    "**fn_build** | BlobField\n",
    "**fn_lose** | BlobField\n",
    "**fn_optimize** | BlobField\n",
    "**fn_train** | BlobField\n",
    "**fn_predict** | BlobField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See [#9.-Algorithm](#9a.-Create) for descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in [Algorithm](#7a.-Create), the `**hp` argument is used to systematically pass hyperparameters into the Algorithm functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given the follow set of hyperparamets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "hyperparameters = dict(\n",
    "    epoch_count     = [30]\n",
    "    , learning_rate = [0.01]\n",
    "    , neuron_count  = [24, 48]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid search would produce the 2 unique `Hyperparamcombo`'s:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[\n",
    "    dict(\n",
    "        epoch_count     = 30\n",
    "        , learning_rate = 0.01\n",
    "        , neuron_count  = 24 #<-- varies\n",
    "    )\n",
    "    \n",
    "    , dict(\n",
    "        epoch_count     = 30\n",
    "        , learning_rate = 0.01\n",
    "        , neuron_count  = 48 #<-- varies\n",
    "    )\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the current value in our model functions like so: `hp['neuron_count']`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10a. Philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is like planning a wedding. First you sort out the big ticket items like the venue, caterer, and band. You go tour and taste a few options. Only after this strategy is in place do you worry about the tactical details like what suit you are going to wear and the songs that will be played.\n",
    "\n",
    "\n",
    "When tuning a neural network:\n",
    "\n",
    "1. First you figure out the high-level architecture: the topologies (types of layers, # of layers, # neurons per layer) and batch size (amount of information).\n",
    "2. Once you find an architecture or two that perform reasonably well, then you can hone in on their nuances like weight initialization, activation methods, and learning rate.\n",
    "\n",
    "Here's why I don't recommend fancy hyperparameter search stragtegies:\n",
    "\n",
    "- Don't combine steps 1 and 2. It's a waste of time to search through the nuances of architectures that were never going to perform well.\n",
    "\n",
    "- The parameters influence each other. If, all of a sudden, I decide that I am going to run with 3 convolutional layers instead of 2, then that completely changes the parameters I want to test next. I might even have parameters that are exclusive to the third layer 3. \n",
    "\n",
    "- If you limit your experiments to tweaking just 1-2 parameters at a time, then it's easy to isolate their effect. The parameters aren't mutually exclusive. If you change the batch size, then it doesn't matter how previous learning rates performed.\n",
    "\n",
    "- It prevents you from learning about how neural networks operate through your own observation. Given enough practice with an architecture, you'll intuitively get a feel for the right balance between \"model complexity and data complexity\" as well as what parameters to tweak in certain situations. You have to become like a coach that assigns specific practice drills to get your team to perform for the task at hand.\n",
    "\n",
    "- Practitioners really only test high/medium/low values for a handful of parameters for a given architecture. If you tell a chef to prepare a special meal for a certain occassion, then he doesn't need a search optimization algorithm to figure out what process and ingredients to use. \n",
    "\n",
    "Now if there was a reinforcement learning (RL) model that looked at the variance in features and produced an architecture-parameter pairing to test - then I would be interested.\n",
    "\n",
    "Never forget, the goal of training is to neurally encode representative information using the simplest topology possible to ensure generalizability. It's a condensed logically representation of a broader reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10b. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Hyperparamset.from_algorithm()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Hyperparamset.from_algorithm(\n",
    "    algorithm_id\n",
    "    , hyperparameters\n",
    "    , search_count\n",
    "    , search_percent\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**algorithm_id** | int | Required | The `Algorithm.id` whose functions these hyperparameters will be used with\n",
    "**hyperparameters** | dict(str:list) | Required | See example in [Hyperparameters](#8.-Hyperparameters). Must be JSON compatible.\n",
    "**search_count** | int | None | Randomly select *n* hyperparameter combinations to test. Must be greater than 1. No upper limit, it will test all combinations if number of combinations is exceeded.\n",
    "**search_percent** | float | None | Given all of the available hyperparameter combinations, search x%. Between `0.0:1.0`. Cannot be used if `search_count` is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Bayesian TPE (Tree-structured Parzen Estimator)\" via `hyperopt` has been suggested as a future area to explore, but it does not exist right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields of the Hyperparamset table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**hyperparamcombo_count** | IntegerField | text\n",
    "**hyperparameters** | JSONField | The original `dict(param=list)` of all possible values\n",
    "**search_count** | IntegerField | The number of randomly selected combinations of hyperparameters\n",
    "**search_percent** | FloatField | The percent of randomly selected combinations of hyperparameters\n",
    "**algorithm** | ForeignKeyField | The `Algorithm.id` whose functions these hyperparameters will be used with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields of the Hyperparamcombo table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**combination_index** | IntegerField | Zero-based counts the number of the number of hyperparamcombos\n",
    "**hyperparameters** | JSONField | The specific combination of hyperparameters that will be fed to the Algorithm functions\n",
    "**hyperparamset** | ForeignKeyField | The Hyperparamset that this combination of hyperparameters was derived from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Queue is the central object of the \"logic side\" of the ORM. It ties together everything we need to run training `Job`'s for hyperparameter tuning. That's why it is referred to as an Experiment in the High-Level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 11a. Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "└── `Queue.from_algorithm()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Queue.from_algorithm(\n",
    "    algorithm_id\n",
    "    , splitset_id\n",
    "    , repeat_count\n",
    "    , permute_count\n",
    "    , hyperparamset_id\n",
    "    , description\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**algorithm_id** | int | Required | The `Algorithm.id` whose functions will be used during training and evaluation\n",
    "**splitset_id** | int | Required | The `Splitset.id` whose samples will be used during training and evaluation\n",
    "**repeat_count** | int | 1 | Each job will be repeat n times. Designed for use with random weight initialization (aka non-deterministic). This is why 1 `Job` has many `Predictors`\n",
    "**permute_count** | int | 3 | Triggers a shuffled permutation of each training data column to determine which columns have the most impact on loss in comparison baseline training loss: `[training loss - (median loss of <n> permutations)]`. The count determines how many times the shuffled permutation is ran before taking the median loss. Permutation does *not* get run on `Feature.dataset.typ=='image'`. Set this to 0 if you do not care about feature importance.\n",
    "**hyperparamset_id** | int | None | The `Hyperparamset.id` whose samples will be used during training and evaluation. This needs to be specified because an Algorithm can have many Hyperparamsets.\n",
    "**description** | str | None | What is unique about this experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "└── `Queue.run_jobs()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs are simply ran on a loop on the main process.\n",
    "\n",
    "Stop the queue with a keyboard interrupt e.g. `ctrl+Z/D/C` in Python shell or `i,i` in Jupyter. It is listening for interupts so it will usually stop gracefully. Even if it errors upon during interrupt, it's not a problem. You can rerun the queue and it will resume on the same job it was running previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Queue.plot_performance()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots every model trained by the queue for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X axis = loss\n",
    "- Y axis = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Queue.plot_performance(\n",
    "    call_display\n",
    "    , max_loss\n",
    "    , min_score\n",
    "    , score_type\n",
    "    , height\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**call_display** | bool | True | If `True`, calls `display()` on plot. If `False`, returns Plotly `figure` object(s).\n",
    "**max_loss** | float | None | Models with any split with higher loss than this threshold will not be plotted.\n",
    "**min_score** | typ | None | Models with any split with a lower score than this threshold will not be plotted.\n",
    "**score_type** | typ | None | Defaults to `\"accuracy\"` for classification analysis, and `\"r2\"` for regression analysis. See [aiqc.utils.meter](https://github.com/aiqc/AIQC/blob/main/aiqc/utils/meter.py) for available metrics.\n",
    "**height** | typ | None | Default height is `560` but you can force it to be taller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Queue.metrics_df()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays metrics for every split/fold of every model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Queue.metrics_df(\n",
    "    selected_metrics\n",
    "    , sort_by\n",
    "    , ascending\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**selected_metrics** | list(str) | None | If you get overwhelmed by the variety of metrics returned, then you can include the ones you want selectively by name.\n",
    "**sort_by** | str | None | You can sort the dataframe by any column name.\n",
    "**ascending** | typ | False | Descending if False.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11c. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields of the Queue table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**repeat_count** | IntegerField | The number of times to repeat each Job.\n",
    "**total_runs** | IntegerField | The total number of models to be trained as a result of this queue being created.\n",
    "**permute_count** | IntegerField | Number of permutations to run on each column before taking the median impact on loss. 0 means permutation was skipped.\n",
    "**runs_completed** | IntegerField | Counts the runs that have actually finished\n",
    "**algorithm** | ForeignKeyField | The model functions to use during training and evaluation\n",
    "**splitset** | ForeignKeyField | The pipeline of samples to feed to the models during training and evaluation\n",
    "**hyperparamset** | ForeignKeyField | Contains all of the hyperparameters to be used for the Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Queue spawns `Job`'s. A Job is like a spec/ manifest for training a model. It may be repeated.\n",
    "\n",
    "`# jobs = Hyperamset.hyperamcombo.count() * Queue.repeat_count * splitset.folds.count()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12a. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields of the Job table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**repeat_count** | IntegerField | The number of times this Job is to be repeated\n",
    "**queue** | ForeignKeyField | The Queue this Job was created by\n",
    "**hyperparamcombo** | ForeignKeyField | The parameters this Job uses\n",
    "**fold** | ForeignKeyField | The cross-validation samples that this Job uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Jobs finish, they save the `model` and `history` metrics within a `Predictor` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13a. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "└── `Predictor.get_model()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Handles fetching and initializing the model (and PyTorch optimizer) from `Predictor.model_file` and `Predictor.input_shapes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "└── `Predictor.get_hyperparameters()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a shortcut to fetch the hyperparameters used to train this specific model. `as_pandas` toggles between `dict()` and `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Predictor.get_hyperparameters(as_pandas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**as_pandas** | bool | True | If `True`, returns a DataFrame. If `False`, returns a `list(dict(param_name=[values]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Predictor.plot_learning_curve()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning curve will be generated for each train-evaluation pair of metrics in the Predictor.history dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Predictor.plot_learning_curve(\n",
    "    skip_head\n",
    "    , call_display\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**skip_head** | bool | True | Skips displaying the first 15% of epochs. Loss values in the first few epochs can often be extremely high before they plummet and become more gradual. This really stretches out the graph and makes it hard to see if the evaluation set is diverging or not. \n",
    "**call_display** | bool | True | If `True`, returns a DataFrame. If `False`, returns a `list(dict(param_name=[values]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13b. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the fields of the Predictor table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute | Type | Description\n",
    "--- | --- | ---\n",
    "**repeat_index** | IntegerField | Counts how many predictors have been trained using a Job spec\n",
    "**time_started** | DateTimeField | When the Job started\n",
    "**time_succeeded** | DateTimeField | When the Job finished\n",
    "**time_duration** | IntegerField | Total time in seconds it took to complete the Job\n",
    "**model_file** | BlobField | Contains a dilled (advanced Pickle) of the trained model. See `Predictor.get_model()` for exporting.\n",
    "**features_shapes** | PickleField | tuple or list of tuples containing the np.shape(s) of feature(s)\n",
    "**label_shape** | PickleField | tuple containing np.shape of a single sample's label\n",
    "**history** | JSONField | Contains the training history loss/metrics\n",
    "**is_starred** | BooleanField | Flag denoting if this model is of interest\n",
    "**job** | ForeignKeyField | The Job that trained this Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 14. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is fed through a Predictor, you get a `Prediction`. During training, Predictions are automatically generated for every split/fold in the `Queue.splitset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14a. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Prediction.calc_featureImportance()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is provided for conducting feature importance after training. It was decoupled from training for the following reasons:\n",
    "\n",
    "- Permutation is computationally expensive, especially for many-columned datasets.\n",
    "- We don't care about the feature importance of poorly performing models, which constitute the vast majority of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    Prediction.calc_featureImportance(permute_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**permute_count** | int | Required | Triggers a shuffled permutation of each training data column to determine which columns have the most impact on loss in comparison baseline training loss: `[training loss - (median loss of <n> permutations)]`. The count determines how many times the shuffled permutation is ran before taking the median loss. Permutation does *not* get run on `Feature.dataset.typ=='image'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Prediction.plot_feature_importance()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots `prediction.feature_importance` if `Queue.permute_count>0` or `Prediction.calc_featureImportance()` was ran after the fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Prediction.plot_feature_importance(\n",
    "    call_display\n",
    "    , top_n\n",
    "    , height\n",
    "    , margin_left\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**call_display** | bool | True | If `True`, returns a DataFrame. If `False`, returns a `list(dict(param_name=[values]))`\n",
    "**top_n** | int | 10 | The number of features to display. If greater than the actual number of features, it just returns all features.\n",
    "**height** | int | None | If `None`, dynamically makes the plot taller to fit all of the columns\n",
    "**margin_left** | int | None | If `None`, dynamically makes the y axis margin wider the longest column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Prediction.plot_roc_curve()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receiver operating curve (ROC) for classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Prediction.plot_roc_curve(call_display)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**call_display** | bool | True | If `True`, returns a DataFrame. If `False`, returns a `list(dict(param_name=[values]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Prediction.plot_precision_recall()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision/recall curve for classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Prediction.plot_precision_recall(call_display)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**call_display** | bool | True | If `True`, returns a DataFrame. If `False`, returns a `list(dict(param_name=[values]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└── `Prediction.plot_confusion_matrix()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices for classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Prediction.plot_confusion_matrix(call_display)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**call_display** | bool | True | If `True`, returns a DataFrame. If `False`, returns a `list(dict(param_name=[values]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14b. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute | Description |\n",
    "| --- | --- | \n",
    "| **predictions** | Decoded predictions ndarray for per split/ fold/ inference |\n",
    "| **feature_importance** | Importance of each column. Only calculated for training split/fold |\n",
    "| **probabilities** | Prediction probabilities per split/ fold. `None` for regression. |\n",
    "| **metrics** | Statistics for each split/fold that vary based on the analysis_type.  |\n",
    "| **metrics_aggregate** | Average for each statistic across all splits/folds. |\n",
    "| **plot_data** | Metrics reformatted for plot functions. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the visualization of performance metrics of `Queue`, `Predictor` and `Prediction` in action -- reference the [Evaluation](evaluation.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
