{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The High-Level API makes it easy to rapidly:\n",
    "\n",
    "* Prepare samples.\n",
    "* Hypertune/ train queues of models. \n",
    "* Feed appropriate data/ parameters into those models.\n",
    "* Evaluate model performance with metrics and plots.\n",
    "\n",
    "It does so by wrapping and bundling together the methods of the [Low-Level API](api_low_level.html). The table below demonstrates how the high-level entities abstract abstract the low-level entities. While this abstraction eliminates many steps to enable rapid model prototyping, it comes at a cost of customization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| High-level object | Groups together or creates the following objects                                                                  |\n",
    "|:-----------------:|:-----------------------------------------------------------------------------------------------------------------:|\n",
    "| `Pipeline`        | Dataset, File, Image, Tabular, Label, Featureset, Splitset, Foldset, Folds, Encoderset, Labelcoder, Featurecoder. |\n",
    "| `Algorithm`       | Functions to build, train, predict, and evaluate a machine learning model.                                        |\n",
    "| `Experiment`      | Algorithm, Hyperparamset, Hyperparamcombos, Queue, Job, Jobset, Result.                                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've already completed the instructions on the [Installation](installation.html) page, then let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiqc\n",
    "from aiqc import datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Tabular Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular/ delimited/ flat-file `Dataset.Tabular` can be created from either Pandas DataFrames or flat files (CSV/ TSV or Parquet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab one of AIQC's built-in datasets from the `datum` module that we imported above. This module is described in the 'Built-In Examples - Datasets' section of the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datum.to_pandas(name='iris.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Pipeline` process starts with raw data. A Dataset object is generated from that data and prepared for training based on the parameters the user provides to the `Pipeline.make` method. To get started, set the `dataFrame_or_filePath` equal to the dataframe we just fetched. It's the only argument that's actually required so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import any scikit-learn encoders that you want to use to encode labels and/ or features. Any encoders that you pass in will need to be instantiated with the attributes you want them to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference the `Encoderset` section of the low-level API for more detail on how to include/ exclude specific `Featureset` columns by name/dtype. The `feature_encoders` argument seen below takes a list of dictionaries as input, where each dictionary contains the `**kwargs` for a `Featurecoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than wrangling your data with many lines of data science code, just set the arguments below and AIQC takes care of the rest: stratification (including continuous dtypes), validation splits, cross-validation folds, and dtype/column specific encoders to be applied on-read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Don't use `fold_count` unless your (total sample count / fold_count) still gives you an accurate representation of your sample population. You can try it with the 'iris_10x.tsv' datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___/ featurecoder_index: 0 \\_________\n",
      "\n",
      "=> The column(s) below matched your filter(s) and were ran through a test-encoding successfully.\n",
      "['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\n",
      "=> Nice! Now all feature column(s) have encoder(s) associated with them.\n",
      "No more Featurecoders can be added to this Encoderset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitset = aiqc.Pipeline.Tabular.make(\n",
    "\tdataFrame_or_filePath = df\n",
    "    , dtype = None\n",
    "\t, label_column = 'species'\n",
    "    , features_excluded = None\n",
    "\t, size_test = 0.24\n",
    "\t, size_validation = 0.12\n",
    "\t, fold_count = None\n",
    "    , bin_count = None\n",
    "    , label_encoder = OneHotEncoder(sparse=False)\n",
    "    , feature_encoders = [{\n",
    "        \"sklearn_preprocess\": PowerTransformer(method='box-cox', copy=False)\n",
    "        , \"dtypes\": ['float64']\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Image Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIQC also supports image data and convolutional analysis. \n",
    "\n",
    "In order to perform *supervised learning* on image files, you'll need both a `Dataset.Image` and a `Dataset.Tabular`:\n",
    "\n",
    "* `Dataset.Image` can be created from either a folder of images or a list of urls. The Pillow library is used to normalize images ingested into AIQC. Each image must be the same size (dimensions) and mode (colorscale).\n",
    "\n",
    "* `Dataset.Tabular` is created as seen in the section above. It must contain 1 row per image.\n",
    "\n",
    "* Then a `Splitset` is constructed using:\n",
    "  * The `Label` of the `Dataset.Tabular`.\n",
    "  * The `Featureset` of the `Dataset.Image`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use the built-in data found in the `datum` module that we imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datum.to_pandas(name='brain_tumor.csv')\n",
    "image_urls = datum.get_remote_urls(manifest_name='brain_tumor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Validating Images üñºÔ∏è: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:32<00:00,  2.42it/s]\n",
      "üñºÔ∏è Ingesting Images üñºÔ∏è: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:26<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "img_splitset = aiqc.Pipeline.Image.make(\n",
    "    folderPath_or_urls = image_urls\n",
    "    , pillow_save = {}\n",
    "    , tabularDF_or_path = df\n",
    "    , tabular_dtype = None\n",
    "    , label_column = 'status'\n",
    "    , label_encoder = None\n",
    "    , size_test = 0.30\n",
    "    , size_validation = None\n",
    "    , fold_count = 4\n",
    "    , bin_count = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the [Compatibility Matrix](compatibility.html), the only library supported at this point in time is `Keras` as it is the most straightforward for entry-level users. \n",
    "\n",
    "> You can find great examples of machine learning cookbooks on this blog: [MachineLearningMastery.com \"Multi-Label Classification\"](https://machinelearningmastery.com/multi-label-classification-with-deep-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define our models, we'll do so by wrapping each phase in the following functions:\n",
    "\n",
    "* `fn_build` contains the topology/ layers.\n",
    "* `fn_train` specifies the samples and how the model should run.\n",
    "* Optional and automatically determined: `fn_optimize`, `fn_predict` and `fn_lose`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can name the functions below whatever you want, but do not change their predetermined arugments (e.g. `features_shape`, `**hp`, `model`, etc.). These items are used behind the scenes to pass the appropriate data, parameters, and models into your training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these are functions, we can even play with the topology as a parameter! As demonstrated by the `if (hp['extra_layer'])` line below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Put a placeholder anywhere you want to try out different hyperparameters: `hp['<some_variable_name>']`. You'll get a chance to define the hyperparameters in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fn_build`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_build(features_shape, label_shape, **hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=features_shape[0], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(hp['dropout_size']))\n",
    "    \n",
    "    if (hp['extra_layer']):\n",
    "        model.add(Dense(units=hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dropout(hp['dropout_size']))\n",
    "    \n",
    "    model.add(Dense(units=label_shape[0], activation='softmax', name='output'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fn_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        samples_train[\"features\"]\n",
    "        , samples_train[\"labels\"]\n",
    "        , validation_data = (\n",
    "            samples_evaluate[\"features\"]\n",
    "            , samples_evaluate[\"labels\"]\n",
    "        )\n",
    "        , verbose = 0\n",
    "        , batch_size = 3\n",
    "        , epochs = hp['epoch_count']\n",
    "        , callbacks=[History()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference the [low-level API documentation](api_low_level.html#Optional,-callback-to-stop-training-early.) for information on the custom 'early stopping' callbacks AIQC makes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hyperparameters` below will be automatically fed into the functions above as `**kwargs` via the `**hp` argument we saw earlier.\n",
    "\n",
    "For example, wherever you see `hp['neuron_count']`, it will pull from the *key:value* pair `\"neuron_count\": [9, 12]` seen below. Where model A will have 9 neurons and model B will have 12 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "\t\"neuron_count\": [9, 12]\n",
    "    , \"extra_layer\": [True, False]\n",
    "\t, \"dropout_size\": [0.10, 0.20]\n",
    "    , \"epoch_count\": [50]\n",
    "    , \"learning_rate\": [0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass these functions into the `Algorithm`.\n",
    "\n",
    "The `library` and `analysis_type` help handle the model and its output behind the scenes. Current analysis types include: 'classification_multi', 'classification_binary', and 'regression'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Experiment.make()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to bring together the data and logic into an `Experiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = aiqc.Experiment.make(\n",
    "    library = \"keras\"\n",
    "    , analysis_type = \"classification_multi\"\n",
    "    , fn_build = fn_build\n",
    "    , fn_train = fn_train\n",
    "    , splitset_id = splitset.id\n",
    "    , repeat_count = 2\n",
    "    , hide_test = False\n",
    "    , hyperparameters = hyperparameters\n",
    "    , foldset_id = None\n",
    "    , encoderset_id = splitset.encodersets[0]\n",
    "    , fn_lose = None #automated.\n",
    "    , fn_optimize = None #automated.\n",
    "    , fn_predict = None #automated.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÆ Training Models üîÆ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:11<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "queue.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on visualization of performance metrics, reference the [Visualization & Metrics](visualization.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
