{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comprehensive-globe",
   "metadata": {},
   "source": [
    "# TensorFlow2: Training Loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-radio",
   "metadata": {},
   "source": [
    "![gradient](../images/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-armstrong",
   "metadata": {},
   "source": [
    "Although Keras is suitable for the vast majority of use cases, in the following scenarios, it may make sense to forgo `model.fit()` to manually define a training loop:\n",
    "\n",
    "- Maintaining legacy code and retraining old models.\n",
    "- Custom batch/ epoch operations like gradients and backpropagation. Even then, PyTorch may be a better fit for customization.\n",
    "\n",
    "> Disclaimer; This notebook demonstrates how to manually define a training loop for queued tuning of a binary classification model. However, it is only included to prove that AIQC technically supports TensorFlow out-of-the-box with `analysis_type='keras'`, and to demonstrate how expert practicioners to do continue to use their favorite tools. We neither claim to be experts on the inner-workings of TensorFlow, nor do we intend to troubleshoot advanced methodologies for users that are in over their heads.\n",
    "\n",
    "Reference this repository for more TensorFlow cookbooks: \n",
    "> https://github.com/IvanBongiorni/TensorFlow2.0_Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90237610-1e04-4eb3-ac99-8c9a3528a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiqc\n",
    "from aiqc import datum\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer, PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-hampton",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-hypothesis",
   "metadata": {},
   "source": [
    "## Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-textbook",
   "metadata": {},
   "source": [
    "Reference [Example Datasets](example_datasets.ipynb) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "round-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datum.to_pandas('sonar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "completed-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th>az</th>\n",
       "      <th>ba</th>\n",
       "      <th>bb</th>\n",
       "      <th>bc</th>\n",
       "      <th>bd</th>\n",
       "      <th>be</th>\n",
       "      <th>bf</th>\n",
       "      <th>bg</th>\n",
       "      <th>bh</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        a       b       c       d       e       f       g       h       i  \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "        j  ...      az      ba      bb      bc      bd      be      bf  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       bg      bh  object  \n",
       "0  0.0090  0.0032       R  \n",
       "1  0.0052  0.0044       R  \n",
       "2  0.0095  0.0078       R  \n",
       "3  0.0040  0.0117       R  \n",
       "4  0.0107  0.0094       R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-bumper",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-rebate",
   "metadata": {},
   "source": [
    "## a) High-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-companion",
   "metadata": {},
   "source": [
    "Reference [High-Level API Docs](api_high_level.ipynb) for more information including how to work with non-tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brave-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___/ featurecoder_index: 0 \\_________\n",
      "\n",
      "=> The column(s) below matched your filter(s) featurecoder filters.\n",
      "\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh']\n",
      "\n",
      "=> Done. All feature column(s) have featurecoder(s) associated with them.\n",
      "No more Featurecoders can be added to this Encoderset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitset = aiqc.Pipeline.Tabular.make(\n",
    "    # --- Data source ---\n",
    "    df_or_path = df\n",
    "    , dtype = None\n",
    "\n",
    "    # --- Label preprocessing ---\n",
    "    , label_column = 'object'\n",
    "    , label_interpolater = None\n",
    "    , label_encoder = dict(sklearn_preprocess = LabelBinarizer(sparse_output=False))\n",
    "\n",
    "    # --- Feature preprocessing ---\n",
    "    , feature_cols_excluded = 'object'\n",
    "    , feature_interpolaters = None\n",
    "    , feature_window = None\n",
    "    , feature_encoders = dict(\n",
    "        sklearn_preprocess = PowerTransformer(method='yeo-johnson', copy=False)\n",
    "        , dtypes = ['float64']\n",
    "    )\n",
    "    , feature_reshape_indices = None\n",
    "\n",
    "    # --- Stratification ---\n",
    "    , size_test = 0.12\n",
    "    , size_validation = 0.22\n",
    "    , fold_count = None\n",
    "    , bin_count = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "undefined-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_build(features_shape, label_shape, **hp):\n",
    "    model = Sequential(name='Sonar')\n",
    "    model.add(Input(shape=features_shape))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(units=label_shape[0], activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tutorial-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_lose(**hp):\n",
    "\tloser = tf.losses.BinaryCrossentropy()\n",
    "\treturn loser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "leading-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimize(**hp):\n",
    "\toptimizer = tf.optimizers.Adamax()\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thermal-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):\n",
    "    batched_train_features, batched_train_labels = aiqc.tf_batcher(\n",
    "        features = samples_train['features']\n",
    "        , labels = samples_train['labels']\n",
    "        , batch_size = 5\n",
    "    )\n",
    "    \n",
    "    # Still necessary for saving entire model.\n",
    "    model.compile(loss=loser, optimizer=optimizer)\n",
    "    \n",
    "    ## --- Metrics ---\n",
    "    acc = tf.metrics.BinaryAccuracy()\n",
    "    # Mirrors `keras.model.History.history` object.\n",
    "    history = {\n",
    "        'loss':list(), 'accuracy': list(), \n",
    "        'val_loss':list(), 'val_accuracy':list()\n",
    "    }\n",
    "\n",
    "    ## --- Training loop ---\n",
    "    for epoch in range(hp['epochs']):\n",
    "        # --- Batch training ---\n",
    "        for i, batch in enumerate(batched_train_features):      \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_loss = loser(\n",
    "                    batched_train_labels[i],\n",
    "                    model(batched_train_features[i])\n",
    "                )\n",
    "            # Update weights based on the gradient of the loss function.\n",
    "            gradients = tape.gradient(batch_loss, model.trainable_variables)            \n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        ## --- Epoch metrics ---\n",
    "        # Overall performance on training data.\n",
    "        train_probability = model.predict(samples_train['features'])\n",
    "        train_loss = loser(samples_train['labels'], train_probability)\n",
    "        train_acc = acc(samples_train['labels'], train_probability)\n",
    "        history['loss'].append(float(train_loss))\n",
    "        history['accuracy'].append(float(train_acc))\n",
    "        # Performance on evaluation data.\n",
    "        eval_probability = model.predict(samples_evaluate['features'])\n",
    "        eval_loss = loser(samples_evaluate['labels'], eval_probability)\n",
    "        eval_acc = acc(samples_evaluate['labels'], eval_probability)\n",
    "        history['val_loss'].append(float(eval_loss))\n",
    "        history['val_accuracy'].append(float(eval_acc))\n",
    "    # Attach history to the model so we can return a single object.\n",
    "    model.history.history = history \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "through-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"neuron_count\": [25, 50]\n",
    "    , \"epochs\": [75, 150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "current-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = aiqc.Experiment.make(\n",
    "    # --- Data source ---\n",
    "    library = \"keras\"\n",
    "    , analysis_type = \"classification_binary\"\n",
    "    \n",
    "    # --- Model functions ---\n",
    "    , fn_build = fn_build\n",
    "    , fn_train = fn_train\n",
    "    , fn_lose = fn_lose\n",
    "    , fn_optimize = fn_optimize\n",
    "    , fn_predict = None #automated\n",
    "\n",
    "    # --- Training options ---\n",
    "    , repeat_count = 1\n",
    "    , hyperparameters = hyperparameters\n",
    "    , search_percent = None\n",
    "    \n",
    "    # --- Data source ---\n",
    "    , splitset_id = splitset.id\n",
    "    , foldset_id = None\n",
    "    , hide_test = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "federal-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”® Training Models ðŸ”®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:18<00:00, 34.72s/it]\n"
     ]
    }
   ],
   "source": [
    "queue.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-senate",
   "metadata": {},
   "source": [
    "For more information on visualization of performance metrics, reference the [Visualization & Metrics](visualization.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-acceptance",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-cheese",
   "metadata": {},
   "source": [
    "## b) Low-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-interstate",
   "metadata": {},
   "source": [
    "Reference [Low-Level API Docs](api_high_level.ipynb) for more information including how to work with non-tabular data and defining optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "herbal-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiqc.Dataset.Tabular.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lightweight-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "induced-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset.make_label(columns=[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "similar-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcoder = label.make_labelcoder(\n",
    "    sklearn_preprocess = LabelBinarizer(sparse_output=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unsigned-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = dataset.make_feature(exclude_columns=[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brown-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderset = feature.make_encoderset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "therapeutic-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___/ featurecoder_index: 0 \\_________\n",
      "\n",
      "=> The column(s) below matched your filter(s) featurecoder filters.\n",
      "\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh']\n",
      "\n",
      "=> Done. All feature column(s) have featurecoder(s) associated with them.\n",
      "No more Featurecoders can be added to this Encoderset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurecoder_0 = encoderset.make_featurecoder(\n",
    "    sklearn_preprocess = PowerTransformer(method='yeo-johnson', copy=False)\n",
    "    , dtypes = ['float64']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "instrumental-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = aiqc.Splitset.make(\n",
    "    feature_ids = [feature.id]\n",
    "    , label_id = label.id\n",
    "    , size_test = 0.22\n",
    "    , size_validation = 0.12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "persistent-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_build(features_shape, label_shape, **hp):\n",
    "    model = Sequential(name='Sonar')\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(units=label_shape[0], activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "regional-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_lose(**hp):\n",
    "\tloser = tf.losses.BinaryCrossentropy()\n",
    "\treturn loser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "biological-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimize(**hp):\n",
    "\toptimizer = tf.optimizers.Adamax()\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "returning-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):\n",
    "    batched_train_features, batched_train_labels = aiqc.tf_batcher(\n",
    "        features = samples_train['features']\n",
    "        , labels = samples_train['labels']\n",
    "        , batch_size = 5\n",
    "    )\n",
    "    \n",
    "    # Still necessary for saving entire model.\n",
    "    model.compile(loss=loser, optimizer=optimizer)\n",
    "    \n",
    "    ## --- Metrics ---\n",
    "    acc = tf.metrics.BinaryAccuracy()\n",
    "    # Mirrors `keras.model.History.history` object.\n",
    "    history = {\n",
    "        'loss':list(), 'accuracy': list(), \n",
    "        'val_loss':list(), 'val_accuracy':list()\n",
    "    }\n",
    "\n",
    "    ## --- Training loop ---\n",
    "    for epoch in range(hp['epochs']):\n",
    "        # --- Batch training ---\n",
    "        for i, batch in enumerate(batched_train_features):      \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_loss = loser(\n",
    "                    batched_train_labels[i],\n",
    "                    model(batched_train_features[i])\n",
    "                )\n",
    "            # Update weights based on the gradient of the loss function.\n",
    "            gradients = tape.gradient(batch_loss, model.trainable_variables)            \n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        ## --- Epoch metrics ---\n",
    "        # Overall performance on training data.\n",
    "        train_probability = model.predict(samples_train['features'])\n",
    "        train_loss = loser(samples_train['labels'], train_probability)\n",
    "        train_acc = acc(samples_train['labels'], train_probability)\n",
    "        history['loss'].append(float(train_loss))\n",
    "        history['accuracy'].append(float(train_acc))\n",
    "        # Performance on evaluation data.\n",
    "        eval_probability = model.predict(samples_evaluate['features'])\n",
    "        eval_loss = loser(samples_evaluate['labels'], eval_probability)\n",
    "        eval_acc = acc(samples_evaluate['labels'], eval_probability)\n",
    "        history['val_loss'].append(float(eval_loss))\n",
    "        history['val_accuracy'].append(float(eval_acc))\n",
    "    # Attach history to the model so we can return a single object.\n",
    "    model.history.history = history \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "typical-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = aiqc.Algorithm.make(\n",
    "    library = \"keras\"\n",
    "    , analysis_type = \"classification_binary\"\n",
    "    , fn_build = fn_build\n",
    "    , fn_train = fn_train\n",
    "    , fn_lose = fn_lose\n",
    "    , fn_optimize = fn_optimize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "immediate-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"neuron_count\": [25, 50]\n",
    "    , \"epochs\": [75, 150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "comfortable-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"neuron_count\": [25, 50]\n",
    "    , \"epochs\": [75, 150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "composite-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamset = algorithm.make_hyperparamset(\n",
    "    hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wrapped-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = algorithm.make_queue(\n",
    "    splitset_id = splitset.id\n",
    "    , hyperparamset_id = hyperparamset.id\n",
    "    , repeat_count = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "advisory-specification",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”® Training Models ðŸ”®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [04:25<00:00, 33.17s/it]\n"
     ]
    }
   ],
   "source": [
    "queue.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-cisco",
   "metadata": {},
   "source": [
    "For more information on visualization of performance metrics, reference the [Visualization & Metrics](visualization.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
