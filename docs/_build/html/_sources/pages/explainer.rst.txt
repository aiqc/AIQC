#################
Deep Learning 101
#################

*Boiling down a neural network to its fundamental concepts.*

----

|

.. raw:: html

  </br>
  <center>
    <h3>üîç Types of Analysis</h3>
  </center>

  </br>
  <p class="explain">
    If you are generally familiar with spreadsheets then you are already half way to understanding AI. For the purpose of this discussion, let's assume that each <i>row</i> in a spreadsheet represents a record, and each <i>column</i> provides information about that record. Bearing this in mind, there are two major types of AI:
  </p>



.. list-table::
  :widths: 15, 85
  :align: center

  * - **Generative**
    - Given what we know about rows 1:1000 ‚Üí generate row 1001.

  * - **Discriminative**
    - Given what we know about columns A:F ‚Üí determine column G's values.

|


.. image:: ../_static/images/explainer/analysis_type.png
  :width: 75%
  :align: center
  :alt: gears
  :class: no-scaled-link

|
|
|

----

|

.. raw:: html
  
  </br>
  <center>
    <h3>üî¨ Subtypes of Analysis</h3>
  </center>

  </br>
  <p class="explain">
    In this tutorial, we will focus on <i>discriminative</i> analysis as it is highly practical and easier to understand. Discrimination helps us answer two important kinds of questions:
  </p>


.. list-table::
  :widths: 15, 10, 12, 60
  :align: center
  
  * - **Categorize**
    - What is it?
    - aka classify
    - e.g. benign vs malignant? which species?

  * - **Quantify**
    - How much?
    - aka regress
    - e.g. price? distance? age? radioactivity?

|

.. image:: ../_static/images/explainer/categorize_quantify.png
  :width: 85%
  :align: center
  :alt: categorize_quantify
  :class: no-scaled-link

|

.. raw:: html
  
  </br>
  <p class="explain">
    Digging one level deeper, there are two types of categorization:
  </p>


.. list-table::
  :widths: 12, 30, 10
  :align: center
  
  * - **Binary**
    - Checking for the presence of a single condition
    - e.g. tumor

  * - **Multi-Label**
    - When there are many possible outcomes
    - e.g. species

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üå°Ô∏è Variables</h3>
  </center>

.. raw:: html

  </br>
  <p class="explain">
    As an example, let's pretend we work at a zoo where we have a spreadsheet that contains information about the traits of different animals. We want to use <i>discriminative learning</i> in order to <i>categorize</i> the species of a given animal.
  </p>


.. list-table::
  :widths: 15, 25, 60
  :align: center
  
  * - **Features**
    - Indepent Variable *(X)*
    - Informative columns like `num_legs`, `has_wings`, `has_shell`.

  * - **Label**
    - Dependent Variable *(y)*
    - The `species` column that we want to predict.

|

.. image:: ../_static/images/explainer/turtle_ruler.png
  :width: 45%
  :align: center
  :alt: turtle_ruler
  :class: no-scaled-link

|

.. raw:: html

  <p class="explain">
    We learn about the <i>features</i> in order to predict the <i>label (aka target)</i>.
  </p>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üç± Stratification</h3>
  </center>

  </br>
  <p class="explain">
    Our predictive algorithm will need samples to learn from as well as samples for <i>evaluating</i> its performance.
  </p>
  </br>
  


.. image:: ../_static/images/explainer/stratification.png
  :width: 85%
  :align: center
  :alt: turtle_ruler
  :class: no-scaled-link


.. raw:: html

  </br></br>
  <p class="explain">
    So we <i>split</i> our dataset into subsets for these purposes. 
    It's important that the distribution of each subset is <i>representatitive</i> of the broader population because we want our algorithm to be able to <i><a href="https://aiqc.medium.com/memorization-isnt-learning-it-s-overfitting-b3163fe6a8b4" target="_blank">generalize</a></i>.
  </p>


.. list-table::
  :widths: 7, 5, 31
  :align: center
  
  * - **Train**
    - 67%
    - What the algorithm is trained on/ learns from.

  * - **Validation**
    - 21%
    - What the model is evaluated against during training.

  * - **Test**
    - 12%
    - Blind *holdout* for evaluating the model at the end of training.

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üìû Encoding & Decoding</h3>
  </center>

  </br>
  <p class="explain">
    Data needs the be numerically <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing" target="_blank">encoded</a> before it can be fed into a neural network. 
  </p>
  </br>


.. list-table::
  :widths: 15, 10, 80
  :align: center

  * - **Binarize**
    - Categorical
    - 1 means presence, 0 means absence.
  
  * - **OneHotEncode(OHE)**
    - Categorical
    - Expand 1 multi-category column into many binary columns.

  * - **Ordinal**
    - Categorical
    - Each category assigned an integer [0,1,2]. Rarely the right approach.

  * - **Scale**
    - Continuous
    - Shrink the range of values between -1:1 or 0:1.

.. raw:: html

  <p class="explain">
    This normalization also <a href="https://towardsdatascience.com/data-leakage-5dfc2e0127d4" target="_blank">helps features start on equal footing and prevent gradient explosion</a>.
  </p>

|

.. image:: ../_static/images/explainer/encode_decode.png
  :width: 50%
  :align: center
  :alt: encoding
  :class: no-scaled-link

|

.. raw:: html
 
  <p class="explain">
    After the algorithm makes its prediction, that information needs to be <i>decoded</i> back into its orginal format
    so that it can be understood by practitioners.
  </p>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>‚öñÔ∏è Algorithm</h3>
  </center>

  </br>
  <p class="explain">
    Now we need an equation (aka <i>algorithm</i> or <i>model</i>) that predicts our <i>label</i> when we show it a set of <i>features</i>. Here is our simplified example:
  </p>
  
|

.. code-block:: python

  species = (num_legs * x) + (has_wings * y) + (has_shell * z)


.. raw:: html

  <p class="figCaption" style="text-align: left;">
    This mock equation is actually identical to a neural network with neither hidden layers nor bias neurons.
  </p>

|

.. raw:: html

  <p class="explain">
    The challenging part is that we need to figure out the right values (aka <i>weights</i>) for the <i>parameters</i> (x, y, z) so that our algorithm makes accurate predictions ‚öñÔ∏è To do this by hand, we would simply use trial-and-error; change the value of <i>x</i>, and then see if that change either improved the model or made it worse.
  </p>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üé¢ Gradient Descent</h3>
  </center>

  </br>
  <p class="explain">
    Fortunately, computers can rapidly perform these repetetitive calculations on our behalf. This is where the magic of AI comes into play. It simply automates that trial-and-error.
  </p>

|

.. image:: ../_static/images/explainer/gradient.png
  :width: 80%
  :align: center
  :alt: gradient


.. raw:: html

  </br>
  <p class="figCaption" style="text-align: left;">
    The figure above demonstrates what happens during a training <i>batch</i>: 
    (1) the algorithm looks at a few rows, 
    (2) makes predictions about those rows using its existing weight, 
    (3) checks how accurate those predictions are, 
    (4) adjusts its weight in an attempt to minimize future errors. 
    It's like finding the bottom of a valley by rolling a ball down it.
    Although this process may be easy for a single weight, imagine optimizing an algorithm that contains thousands or millions of different weight parameters.
  </p>
  </br>

|

.. image:: ../_static/images/explainer/memory_foam.png
  :width: 35%
  :align: center
  :alt: memory_foam
  :class: no-scaled-link

|

.. raw:: html

  <p class="explain">
    With repetition, the model molds to the features like a memory foam mattress. For example, you could use the imprint above as a binary classifier for a hand versus other body parts.
  </p>
  
|
|
  
----

|

.. raw:: html
  
  </br>
  <center>
    <h3>üèõÔ∏è Architectures</h3>
  </center>

  </br>
  <p class="explain">
    There are different types of algorithms (aka neural network architectures)<br/> for working with different types of data:
  </p>
  <br/>


.. list-table::
  :widths: 24, 38, 38
  :align: center
  
  * - **Linear**
    - üßÆ Tabular
    - e.g. spreadsheets & tables.

  * - **Convolutional**
    - üì∏ Spatial/ Positional
    - e.g. images, videos, & networks.

  * - **Recurrent**
    - ‚è±Ô∏è Ordered/ Positional
    - e.g. time, text, & DNA.


.. raw:: html

  </br>
  <p class="explain">
    They can be mixed and matched to handle almost any real-life scenario.
  </p>

|
|

----

|

.. raw:: html
  
  </br>
  <center>
    <h3>üï∏Ô∏è Networks</h3>
  </center>

  </br>
  <p class="explain">
    Graph theory is a mathematical discipline that represents connected objects as networks comprised of:
  </p>


.. list-table::
  :widths: 12, 12, 40, 21
  :align: center

  * - **Nodes**
    - neurons
    - participants in the network
    - üí° e.g. lightbulbs

  * - **Edges**
    - weights
    - connect (aka link) the nodes together
    - üîå e.g. wires

|

.. image:: ../_static/images/explainer/lights.png
  :width: 50%
  :align: center
  :alt: lights
  :class: no-scaled-link

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üó∫Ô∏è Topology</h3>
  </center>

  </br>
  <p class="explain">
    The structure of the neural network is referred to as the <i>topology</i>.
    The diagram below shows the topology of a linear architecture.
    Although it may seem overwhelming at first, the mechanisms of the individual components are actually quite simple.
    We'll start with the big picture and then deconstruct it to understand what each piece does. 
  </p>

|

.. image:: ../_static/images/explainer/topology_rectangle.png
  :width: 95%
  :align: center
  :alt: oz
  :class: no-scaled-link

|

.. raw:: html

  <p class="figCaption" style="text-align: left;">
    Running with our network analogy - as data passes through each wire, it is multiplied by that wire's adjustable <i>weight</i> that we mentioned previously.
    In this way, the weights act like amplifiers that adjust the voltage passing through the network.
    Meanwhile, the neurons are like lightbulbs with degrees of brilliance based on the strength of the voltage they receive from all of their incoming wires.
    The <i>bias</i> neurons don't actually touch the data. They act like a y-intercept (think <i>b</i> in <i>y = mx + b</i>).
  </p>

|

.. raw:: html

  <p class="explain">
    If things still aren't making sense, try thinking of a neural network as a galtonboard (aka "bean machine"). The goal is to shape the <i>topology</i> 
    of the network so that it can successfully tease apart the patterns in the data to make the right predictions.
  </p>

  <br/>

  <iframe style="margin:auto;display:block;" width="66%" height="350" src="https://www.youtube.com/embed/S5WAswaJRjw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
  </iframe>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üßÖ Layers</h3>
  </center>

  </br>
  <p class="explain">
    Within a neural network, there are different types of <i>layers</i>:
  </p>


.. list-table::
  :widths: 13, 50
  :align: center
  
  * - **Input**
    - Receives the data. Mirrors the shape of incoming features (e.g. 30 features means 30 input neurons).

  * - **Hidden**
    - Learns from patterns in the features. The number of layers and the amount of neurons that they contain corresponds to the complexity of features.

  * - **Output**
    - Compares predictions to the real label. Mirrors shape of labels (e.g. 5 categories means 5 output neurons, except for binary classification and regression which both use a single output neuron).
  
  * - **Regulatory**
    - [Not pictured here] *Dropout, BatchNorm, MaxPool* help keep the network balanced and help prevent overfitting.

|

.. image:: ../_static/images/explainer/layers.png
  :width: 30%
  :align: center
  :alt: layers
  :class: no-scaled-link

|

.. raw:: html

  <p class="explain">
    Coming back to our zoo example, the number of input neurons in our input layer would be equal to the number of features, 
    and the number of output neurons in our output layer would be equal to the number of possible species.
  </p>
  </br>
  <p class="explain">
   The number of hidden layers and the amount of hidden neurons in those layers will vary based on the complexity of the 
   problem we are trying to solve ü¶è Classifying rhinos vs mosquitoes based on their weight is such a simple task that it would 
   not require any hidden layers at all üêÜ However, delineating the subtle differences between types of big cats 
   (lynx, ocelot, tiger, cougar, panther) may require several layers in order to tease apart their differences.
   For example, upon examining the activation values of a trained network, we may find that the first hidden layer checks for the pattern of the fur (e.g. spotted vs striped), while the second hidden layer determines the color of these markings.
  </p>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üß† Biological Neurons</h3>
  </center>

  </br>
  <p class="explain">
    How does a neuron in the brain process information? 
  </p>

.. image:: ../_static/images/explainer/neuron.png
  :width: 60%
  :align: center
  :alt: neuron
  :class: no-scaled-link

|

.. raw:: html

  <p class="explain">
    In the brain, networks of neurons work together to respond to an incoming stimulus. 
    They repeatedly pass information to downstream neurons in the form of neurotransmitters.
  </p>
  </br>
  <p class="explain">
    However, neurons only forward information if certain conditions are met.
    As the neuron receives incoming signals, it builds up an electrically charged chemical concentration (aka <i>action potential</i>) inside its cell membrane. When this concentration exceeds a certain threshold, it fires a <i>spike</i>. 
  </p>

|

.. image:: ../_static/images/explainer/squid.png
  :width: 100%
  :align: center
  :alt: squid
  :class: no-scaled-link


.. raw:: html

  </br>
  <p class="figCaption" style="text-align: left;">
    The Hodgkin-Huxley duo discovered these phenomena by observing the rate at which neurons in giant squids fired spikes based on variations in current ü¶ë
    <i>Trappenburg, Fundamentals of Computational Neuroscience. 2nd edition, 2010</i>
  </p>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>‚ö° Artificial Neurons</h3>
  </center>

  </br>
  <p class="explain">
    How does an artificial neuron process information?
  </p>

  <p class="explain">
   Similar to how a biological neuron accumulates an <i>action potential</i> based on input from preceding neurons - 
   an artificial neuron accumulates a <i>weighted sum</i> by adding up all of the activation values of its incoming edges.
  </p>

|

.. image:: ../_static/images/explainer/activation.png
  :width: 60%
  :align: center
  :alt: activation
  :class: no-scaled-link

|

.. raw:: html

  <p class="explain">
    How then is the spiking threshold for artificial neurons determined? Any way we program it! 
    The weighted sum can be ran through any <i>activation function</i> of our choosing.
  </p>
  </br></br>
  <p class="explain">
    Different layers make use of different activation functions:
  </p>


.. list-table::
  :widths: 12, 88
  :align: center
  
  * - **Input**
    - In a linear network, the receiving layer does not have an activation function.

  * - **Hidden**
    - The de facto activation function is *ReLU*. Rarely, *Tanh*.

  * - **Output**
    - *Sigmoid* for binary classify. *Softmax* for multi-label classify. None for regression.

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üìà Performance Metrics</h3>
  </center>
  
  </br>
  <p class="explain">
    How does the algorithm know if its predictions are accurate? As mentioned in the sections above,
    it calculates the difference between its predicted label and the actual label. There are different
    strategies for calculating this <b><a href="https://keras.io/api/losses/" target="_blank">loss</a></b>:
  </p>


.. list-table::
  :widths: 35, 25
  :align: center
  
  * - **BinaryCrossentropy**
    - Binary classification.

  * - **CategoricalCrossentropy**
    - Multi-label classification.

  * - **MeanSquaredError or MeanAbsoluteError.**
    - Used for regression.

|

.. raw:: html

  <p class="explain">
   Although neural networks are great at minimizing loss, this metric is hard for humans to understand.
   The following two <b><a href="https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics">metrics</a></b>
   are more intuitive because they have a maximum of 100% and a minimum of 0% (aka normalized unit interval).
  </p>


.. list-table::
  :widths: 12, 15 
  :align: center
  
  * - **Accuracy**
    - Classification.

  * - **R¬≤**
    - Regression.

|

.. raw:: html

  <p class="explain">
    A <i>learning curve</i> keeps track of these metrics over the course of model training:
  </p>


.. image:: ../_static/images/explainer/classify_learn.png
  :width: 90%
  :align: center
  :alt: learning_curve
  :class: no-scaled-link

|

.. raw:: html

  <p class="explain">
    Have a look at the other <a href='../notebooks/evaluation.html' target="_blank">visualizations & metrics</a>
     provided by AIQC.
  </p>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üéõÔ∏è Tuning</h3>
  </center>

  </br>
  <p class="explain">
    A data scientist oversees the training of an neural network much like a chef prepares a meal. The heat is what actually cooks the food, but there are still a few things that the chef controls:
  </p>


.. list-table::
  :widths: 18, 80
  :align: center

  * - **Topology**
    - If the food doesn't fit in the pot, switch to a pot with deeper/ taller *layers*.

  * - **Duration**
    - Food isn't fully cooked? Train for more *epochs* or decrease the size of each *batch*.
  
  * - **Parameters**
    - Burning? Turn down *learning rate*. Flavor is slightly off? Try *initialization*/ *activation* spices.

  * - **Regulation**
    - Overfitting on the same old recipes? Add *Dropout* to mix things up.

|

.. image:: ../_static/images/explainer/cooking.png
  :width: 55%
  :align: center
  :alt: cooking
  :class: no-scaled-link

|

.. raw:: html
  
  </br>
  <p class="explain">
    At first, the number of options to <i>tune</i> seems overwhelming, but you quickly realize that you only need to learn a handful of common dinner <a href='gallery.html'>recipes</a> in order to get by.
  </p>

|
|

----

|

.. raw:: html

  </br>
  <center>
    <h3>üöÄ Let's Get Started</h3>
  </center>

  </br>
  <p class="explain">
    Now you're ready to start your own experiments with the help of the tutorials. The rest is just figuring out how to feed your data into and out of the algorithms, 
    which is where the <a href='../index.html' target="_blank">AIQC</a> framework comes into play.
  </p>
  </br>


.. image:: https://imgs.xkcd.com/comics/machine_learning.png
  :width: 40%
  :align: center
  :alt: oz
  :class: no-scaled-link


.. raw:: html

  </br>
  <center>
    <small style="color:gray">
      <i>The classic <a href="https://xkcd.com/1838/" target="_blank">xkcd</a> comic.</i>
    </small>

    </br></br></br></br>
    <a href="gallery.html">
      <div class="bttn"><b>‚Üí</b> <span class="textz">Use Cases & Tutorials</span></div>
    </a>
  </center>


  <script>
    window.addEventListener('load', function() {
      var art = document.querySelector("div[itemprop='articleBody']")
      art.style.borderRadius = "25px";
      art.style.background = "#ffffff"; 
      art.style.padding = "25px";
    });
  </script>

