{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've already completed the instructions on the **Installation** page, then let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiqc\n",
    "from aiqc import datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingest a `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Relational Model (ORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, AIQC supports the following types of Datasets:\n",
    "\n",
    "* Single-file tabular/ flat/ delimited datasets.\n",
    "* Multi-file image datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End users only need to worry about passing the right inputs to the Dataset class, but there are a few objects doing the legwork beneath the hood:\n",
    "\n",
    "* `Dataset` ORM class with subclasses of either `Tabular` or `Image`.\n",
    "  * `File` ORM class one or more files with subclasses of either `Tabular` or `Image`.\n",
    "    * Dedicated `Tabular` and `Image` ORM classes for attributes specific to those data types (e.g. dtype mappings for flat files and colorscale for images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Considering these types in the future: Sequence/ time series: multi-file tabular (e.g. 3D numpy, HDF5). Graph: multi-file nodes and multi-file edges (e.g. DGL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting and Compressing Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the actual bytes of the file are persisted to the SQLite `BlobField`. It gets gzip compressed, reducing the size by up to 90%. Maximum BlobField size is 2.147 GB, but once you factor in compression, your bottleneck is more likely to be memory beyond that size. The bytes themselves are Parquet (single-partitioned) because, using the PyArrow engine, it preserves every dtype except certain datetimes (which are honestly better off parsed into floats/ ordinal ints). Parquet is also integrated nicely into both Spark and Dask; frameworks for distributed, in-memory computation.\n",
    "\n",
    "Persisting the file ensures reproducibility by: (a) keeping the data packaged alongside the experiment, and (b) helping entry-level users move away from relying on mutable dataframes they have had in-memory for extended periods of time or floating around on shared file systems.\n",
    "\n",
    "> *However, we realize that a different approach will be required at scale, so the `source_path` of the file is recorded whenever possible. In the future we could just read the data from that path (e.g. NFS, RDBMS, HDFS, S3) if the BlobField is none. Or just switch our data fetching/ filtering to Dask because it uses the Pandas API and Parquet.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make a dataset from either:\n",
    "\n",
    "* `Dataset.Tabular`\n",
    "\n",
    "  * In-memory data structures (pandas dataframe, numpy array).\n",
    "  \n",
    "  * Flat files (csv, tsv, parquet).\n",
    "  \n",
    "    * Accepts urls.\n",
    "\n",
    "\n",
    "* `Dataset.Image`\n",
    "\n",
    "  * Any image file format that can be read by the Pillow library.\n",
    "  \n",
    "    * Accepts urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dataset.Tabular.from_pandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datum.to_pandas('iris.tsv')\n",
    "\n",
    "dataset = aiqc.Dataset.Tabular.from_pandas(\n",
    "\tdataframe = df\n",
    "    , name = 'tab separated plants'\n",
    "    , dtype = None #passed to pd.Dataframe(dtype)/ inferred\n",
    "    , column_names = None #passed to pd.Dataframe(columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optionally, `dtype`, as seen in the `pandas.DataFrame.astype(dtype)` [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html), can be specified as either a single type for all columns, or as a dictionary that maps a specific type to each column name. This encodes features for analysis. We read NumPy into Pandas before persisting it, so `columns` and `dtype` are read directly by `pd.DataFrame()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dataset.Tabular.from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must be a 2D NumPy N-Dimensional Array.\n",
    "\n",
    "> *In the future, we may add support for ingesting 3D arrays as multi-file sequences.*\n",
    "\n",
    "Regular *ndarrays* don't have column names, and I didn't like the API for *structured arrays* so you have to pass in columns names as a list. If you don't then column names will be numerically assigned in ascending order (zero-based index), but I didn't like the range object, so I stringified numerically assigned columns to string-based numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr =  df.to_numpy()\n",
    "cols = list(df.columns)\n",
    "\n",
    "other_dataset = aiqc.Dataset.Tabular.from_numpy(\n",
    "\tndarray = arr\n",
    "    , name = None\n",
    "    , dtype = None #passed to pd.Dataframe(dtype)/ inferred \n",
    "    , column_names = cols #passed to pd.Dataframe(columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dataset.Tabular.from_path`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intended for flat files, delimited text, and structured tabular data. It's read in via Pandas, so it supports URLs to raw data and bytes as well.\n",
    "\n",
    "The `file_path` itself can be either absolute or relative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = datum.get_path('iris_10x.tsv')\n",
    "\n",
    "# We'll keep this larger dataset handy for `Foldset` creation later.\n",
    "big_dataset = aiqc.Dataset.Tabular.from_path(\n",
    "    file_path = file_path\n",
    "    , source_file_format = 'tsv'\n",
    "    , name = None\n",
    "    , dtype = None\n",
    "    , column_names = None\n",
    "    , skip_header_rows = 'infer' #passed to `pd.read_csv(header)`. Incompatible w Parquet.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you leave `name` blank, it will default to a human-readble timestamp with the appropriate file extension (e.g. '2020_10_13-01_28_13_PM.tsv')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image datasets are somewhat multi-modal in that, in order to perform supervised learning on them, they require a loosely coupled `Dataset.Tabular` that contains their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/aiqc/aiqc/blob/file_schema/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/aiqc/aiqc/blob/file_schema/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/aiqc/aiqc/blob/file_schema/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/aiqc/aiqc/blob/file_schema/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/aiqc/aiqc/blob/file_schema/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status                                                url\n",
       "0       0  https://github.com/aiqc/aiqc/blob/file_schema/...\n",
       "1       0  https://github.com/aiqc/aiqc/blob/file_schema/...\n",
       "2       0  https://github.com/aiqc/aiqc/blob/file_schema/...\n",
       "3       0  https://github.com/aiqc/aiqc/blob/file_schema/...\n",
       "4       0  https://github.com/aiqc/aiqc/blob/file_schema/..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = datum.to_pandas(name='brain_tumor.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `['status']` column of this dataframe serves as the Label of that sample. We'll construct a `Dataset.Tabular` from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_dataset = aiqc.Dataset.Tabular.from_pandas(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_label = tabular_dataset.make_label(columns=['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dataset.Image.from_urls()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During ingestion, all image files must have the same `Image.mode` and `Image.size` according to the Pillow library.\n",
    "\n",
    "> https://pillow.readthedocs.io/en/stable/handbook/concepts.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_urls(urls:list)` needs a list of urls. In order to perform supervised learning, the order of this list must line up with the samples in your Tabular dataset.\n",
    "> We happen to have this list prepared in the `['url']` column of the dataframe above.  acts as a manifest in that it contains the URL of the image file for that sample, solely for the purposes of initial ingestion. We'll construct a `Dataset.Image` from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = datum.get_remote_urls(manifest_name='brain_tumor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖼️ Validating Images 🖼️: 100%|███████████████████████| 80/80 [00:34<00:00,  2.32it/s]\n",
      "🖼️ Ingesting Images 🖼️: 100%|████████████████████████| 80/80 [00:24<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "image_dataset = aiqc.Dataset.Image.from_urls(urls=image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_featureset = image_dataset.make_featureset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping forward a bit, we bring the heterogenous `Featureset` and `Label` together in the `Splitset`, and they can be used as normal. You can even construct a `Foldset` from this splitset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_splitset = image_featureset.make_splitset(\n",
    "    label_id = tabular_label.id\n",
    "    , size_test = 0.24\n",
    "    , size_validation = 0.12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dataset.Image.from_folder()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading images from a locally accessible folder, the fantastic `natsort.natsorted` library is used as the source of truth for the order of the files.\n",
    "> Python reads files by insertion order rather than alpha-numerically, which isn't intuitive for humans. So make sure your tabular manifest has the same order as `natsorted`. https://natsort.readthedocs.io/en/master/api.html#natsort.natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖼️ Validating Images 🖼️: 100%|█████████████████████| 80/80 [00:00<00:00, 2670.23it/s]\n",
      "🖼️ Ingesting Images 🖼️: 100%|███████████████████████| 80/80 [00:00<00:00, 232.15it/s]\n"
     ]
    }
   ],
   "source": [
    "image_dataset = aiqc.Dataset.Image.from_folder(\"/Users/layne/desktop/brain_tumor_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the first 3 files that comprise that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<File: 85>, <File: 86>, <File: 87>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset.files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_featureset = image_dataset.make_featureset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_splitset = image_featureset.make_splitset(\n",
    "    label_id = tabular_label.id\n",
    "    , size_test = 0.24\n",
    "    , size_validation = 0.12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the sample-related objects in the API have `to_numpy()` and `to_pandas()` methods that accept the following arguments:\n",
    "\n",
    "* `samples=[]` list of indices to fetch.\n",
    "* `columns=[]` list of columns to fetch.\n",
    "* In some cases you can specify a `split`/ `fold` name.\n",
    "\n",
    "For structured data, since the `Dataset` itself is fairly removed from the `File.Tabular` it creates, you can get that tabular file with `Dataset.Tabular.get_main_tabular(dataset_id)` to inspect attributes like `dtypes` and `columns`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we'll see how these arguments allow downstream objects like `Splitset` and `Foldset` to slice up the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset.Tabular.to_pandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0\n",
       "4           5.0          3.6           1.4          0.2        0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal_length  sepal_width\n",
       "0            5.1          3.5\n",
       "13           4.3          3.0\n",
       "29           4.7          3.2\n",
       "79           5.7          2.6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = aiqc.Dataset.to_pandas(\n",
    "    id = dataset.id \n",
    "    , samples = [0,13,29,79]\n",
    "    , columns = ['sepal_length', 'sepal_width']\n",
    ")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset.Tabular.to_numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.1, 0.1],\n",
       "       [1.6, 0.2],\n",
       "       [3.5, 1. ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = dataset.to_numpy(\n",
    "    samples = [0,13,29,79] \n",
    "    , columns = ['petal_length', 'petal_width']\n",
    ")\n",
    "arr[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 0. ],\n",
       "       [4.9, 3. , 1.4, 0.2, 0. ],\n",
       "       [4.7, 3.2, 1.3, 0.2, 0. ],\n",
       "       [4.6, 3.1, 1.5, 0.2, 0. ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = aiqc.Dataset.to_numpy(id=dataset.id)\n",
    "arr[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset.Image.to_pillow()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a list of `PIL.Image`'s. You can actually see the image when you call them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=120x160 at 0x14ABD1A90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_pillow = aiqc.Dataset.Image.to_pillow(id=image_dataset.id, samples=[60,61,62])\n",
    "images_pillow[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset.Image.to_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simply performs `np.array(Pillow.Image)`. Returns an N-dimensional array where the dimensions vary based on the `mode` aka colorscale of the image. For example, it returns '3D of 2Ds for black and white' or '4D of 3Ds for colored' - which would change the class of convultional layer you would use (`Conv1D`:`Conv3D`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 11, 11, ..., 18, 18, 18],\n",
       "       [ 4,  4,  4, ..., 18, 18, 18],\n",
       "       [ 0,  0,  0, ..., 18, 18, 18],\n",
       "       ...,\n",
       "       [18, 18, 18, ..., 18, 18, 18],\n",
       "       [18, 18, 18, ..., 18, 18, 18],\n",
       "       [18, 18, 18, ..., 18, 18, 18]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_pillow = aiqc.Dataset.Image.to_numpy(id=image_dataset.id, samples=[60,61,62])\n",
    "images_pillow[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the moment, we haven't found it necessary to provide a `to_pandas` method for images as they have no need for column names, the dtypes are homogenous, images are used as a whole so there is no filtering, Pandas isn't great with 3D data, and Pillow is integrated with NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select the `Label` column(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Dataset, pick the column(s) that you want to predict/ train against. Creating a `Label` won't duplicate your data! It simply marks the Dataset `columns` to be used for supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we'll see that a `Label` triggers:\n",
    "\n",
    "* The `supervision` attribute of a `Splitset` to be either `'unsupervised'`/`'supervised'`.\n",
    "\n",
    "* Approval/ rejection of the `Algorithm.analysis_type`. For example, you wouldn't perform regression on a string label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the magic of this library is that it prevents you from making silly mistakes like these so that you aren't faced with some obscure NumPy/ Tensor, dtype/ dimensionality error on the nth layer of your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical labels, but not for continuous/float labels, the `Label.unique_classes` are recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the name of the label column handy as you may want to re-use it later when excluding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'species'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset.make_label(columns=[label_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `columns=[label_column]` is a list in case users have already OneHotEncoded (OHEd) their label. If multiple columns are provided, then they must already be in OHE format. I'm not keen on supporting multi-label/ simultaneous analysis, but that could changed based on feasibility and user demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_label = aiqc.Label.from_dataset(\n",
    "\tdataset_id=other_dataset.id\n",
    "\t, columns=[label_column]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Label` comes in handy when we need to fetch what is traditionally referred to as '*Y*' in tutorials. It also accepts a `samples` argument, so that `Splitset` can subset it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     species\n",
       "145        2\n",
       "146        2\n",
       "147        2\n",
       "148        2\n",
       "149        2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.to_pandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.to_numpy(samples=[0,33,66,99,132])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select the `Featureset` column(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Featureset won't duplicate your data! It simply records the Dataset `columns` to be used as features during training. \n",
    "\n",
    "There are three ways to define which columns you want to use as features:\n",
    "\n",
    "- `exclude_columns=[]` e.g. use all columns except the label column.\n",
    "- `include_columns=[]` e.g. only use these columns that I think are informative.\n",
    "- Leave both of the above blank and all columns will be used (e.g. images or unsupervised leanring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For structured data, since the Featureset is far removed from the `File.Tabular` that it is derived from, there is a `Featureset.get_dtypes()` method. This will come in handy when we are selecting dtypes/columns to include/ exclude in our `Featurecoder`(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via `include_columns=[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_columns = [\n",
    "    'sepal_length',\n",
    "    'petal_length',\n",
    "    'petal_width'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = dataset.make_featureset(include_columns=include_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via `exclude_columns=[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = dataset.make_featureset(exclude_columns=[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way, any excluded columns will be recorded since they are used for dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['species']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.columns_excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for images, just perform `Dataset.Image.make_featureset()` since you'll likely want to include all pixels and your label column is in a separate, coupled Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.to_numpy()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal_length  sepal_width  petal_length  petal_width\n",
       "0            5.1          3.5           1.4          0.2\n",
       "16           5.4          3.9           1.3          0.4\n",
       "32           5.2          4.1           1.5          0.1\n",
       "64           5.6          2.9           3.6          1.3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset.to_pandas(samples=[0,16,32,64]).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Slice samples with a `Splitset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Splitset` divides a the samples of the Dataset into the following *splits* in the table below. It is the central object of the data preparation side of the ORM in that it touches `Label`, `Featureset`, `Foldset`, and `Encoderset`. It is the only mandatory data preparation object required by the training `Queue`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both contiuous and categorical `Labels` are automatically stratified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Split                 | Description                                                                                                                                                                                             |\n",
    "|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| train                 | The samples that the model will be trained upon. <br/>Later, we’ll see how we can make *cross-folds from our training split*. <br/>Unsupervised learning will only have a training split.                 |\n",
    "| validation (optional) | The samples used for training evaluation. <br/>Ensures that the test set is not revealed to the model during training.                                                                                  |\n",
    "| test (optional)       | The samples the model has never seen during training. <br/>Used to assess how well the model will perform on unobserved, natural data when it is applied in the real world aka how generalizable it is. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, creating a Splitset won't duplicate your data. It simply denotes the sample indices (aka rows) to be used in the splits that you specify!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Default supervised 70-30 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only provide a Label, then 70:30 train:test splits will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = featureset.make_splitset(label_id=label.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Specifying test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = featureset.make_splitset(\n",
    "\tlabel_id = label.id\n",
    "\t, size_test = 0.30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Specifying validation size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = featureset.make_splitset(\n",
    "\tlabel_id = label.id\n",
    "\t, size_test = 0.20\n",
    "\t, size_validation = 0.12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Taking the whole dataset as a training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset_unsupervised = featureset.make_splitset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Label-based stratification is used to ensure equally distributed label classes for both categorical and continuous data.\n",
    ">\n",
    "> If you want more control over stratification of continuous splits, specify the number of `continuous_bin_count` for grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Stratification of continuous labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All splits are stratified by default in that they contain similar distributions of unique label classes so that each split is a statistically accurate representation of the population as a whole.\n",
    "\n",
    "In order to support this process for continuous labels, binning/ discretization is utilized. For example, if 4 bins are used, values from *0.0 to 1.0* would be binned as *[0.0-0.25, 0.25-0.50, 0.50-0.75, 0.75-1.0]*. This is controlled by the `make_splitset(bin_count:int)` argument. \n",
    "\n",
    "> Reference the handy `Pandas.qcut()`  and the source code `pd.qcut(x=array_to_bin, q=bin_count, labels=False, duplicates='drop')` for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Splitsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validation', 'train', 'test'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitset.samples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.keys()` of 1st layer are referred to as \"split_name\" in the source code: e.g. 'train' as well as, optionally, 'validation' and 'test'.\n",
    "  \n",
    "`Splitset.samples` on disk:\n",
    "```\n",
    " {\n",
    "     'train': [<sample_indices>],\n",
    "     'validation': [<sample_indices>],\n",
    "     'test': [<sample_indices>]\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also verify the actual size of your splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': {'percent': 0.12, 'count': 18},\n",
       " 'test': {'percent': 0.2, 'count': 30},\n",
       " 'train': {'percent': 0.68, 'count': 102}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitset.sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main attribute of the splitset is the `samples` dictionary. Again, on-disk this only contains sample indices. The dictionary is structured like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Splitset.to_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fetched to memory, the `.keys()` of the 2nd layer are: 'features' and, optionally, 'labels'.\n",
    "\n",
    "Note that if you do not specified neither a `size_validation` nor `size_test`, then your dictionary will contain neither a `['validation']` nor `['test']` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.5, 1.3, 0.3],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [4.5, 2.3, 1.3, 0.3]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitset.to_numpy()['train']['features'][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Splitset.to_pandas()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting more fine-tuned, both the numpy and pandas methods support a few optional filters for the sake of memory-efficiency when fetching larger splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, imagine you are fetching data to specifically encode the only float column in the featureset of the test split. You don't need the labels and you don't need the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_width\n",
       "35           3.2\n",
       "113          2.5\n",
       "78           2.9\n",
       "128          2.8\n",
       "133          2.8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitset.to_pandas(\n",
    "\tsplits = ['test']\n",
    "\t, include_label = False\n",
    "\t, include_featureset = True\n",
    "\t, feature_columns = ['sepal_width']\n",
    ")['test']['features'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optionally, create a `Foldset` for cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reference the [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html) to learn more about folding.*\n",
    "\n",
    "![Cross Folds](../images/cross_fold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the left out fold (blue) as the `fold_validation` and the remaining training data as the `folds_train_combined` (green)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *In the future, we may introduce more folding `strategies` aside from leave-one-out.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Fold` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of determining which samples get trained upon, the only thing that matters is the slice of data that gets left out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip - DO NOT use a `Foldset` unless your *(total sample count / fold_count)* still gives you an accurate representation of your sample population. If you are ignoring that advice and stretching to perform cross-validation, then at least ensure that *(total sample count / fold_count)* is evenly divisible. Both of these tips help avoid poorly stratified/ undersized folds that perform either too well (only most common label class present) or poorly (handful of samples and a few inaccurate prediction on a normally good model).\n",
    ">\n",
    "> Tip - The sample indices of the validation fold are not discarded. In fact, `fold_validation` can actually be used alongside a split `validation` for double validation 🤘. However, it's more sensible to skip the validation split when cross-validating because you'll want each `fold_validation` to be as large (representative of the population) as possible. Folds naturally have fewer samples, so a handful of incorrect predictions have the potential to offset your aggregate metrics.\n",
    "> \n",
    "> Candidly, if you've ever performed cross-validation manually, let alone systematically, you'll know that, barring stratification of continuous labels, it's easy enough to construct the folds, but then it's a pain to generate performance metrics (e.g. `zero_division`, absent OHE classes) due to the absence of outlying classes and bins. Time has been invested to handle these scenarios elegantly so that folds can be treated as first-class-citizens alongside splits. That being said, if you try to do something undersized like \"150 samples in their dataset and a `fold_count` > 3 with `unique_classes` > 4,\" then you may run into edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `Splitset.samples`, there is a `Fold.samples` dictionary of sample indices with the following `.keys()`:\n",
    "* `samples['folds_train_combined']` - all the included folds.\n",
    "* `samples['fold_validation']` - the fold that got left out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross fold objects](../images/cross_fold_objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Foldsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_label = big_dataset.make_label(columns=[label_column])\n",
    "big_fset = big_dataset.make_featureset(exclude_columns=[label_column])\n",
    "big_splits = big_fset.make_splitset(\n",
    "\tlabel_id = big_label.id\n",
    "\t, size_test = 0.30\n",
    "    , bin_count=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to generate 5 `Fold` objects that belong to the `Foldset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset = big_splits.make_foldset(fold_count=5, bin_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Fold: 1>, <Fold: 2>, <Fold: 3>, <Fold: 4>, <Fold: 5>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(foldset.folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Foldsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample indices of each Fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 11]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset.folds[0].samples['folds_train_combined'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 16, 19, 23, 25, 27, 28, 32, 38]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset.folds[0].samples['fold_validation'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Foldset.to_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce memory footprint the `to_numpy()` and `to_pandas()` methods introduce the `fold_index` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no fold_index is specified, then it will fetch all folds and give each fold a numeric key according to its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "So you need to specify the `fold_index` as the first key when accessing the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.8, 1.5, 0.3]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset.to_numpy(fold_index=0)[0]['fold_validation']['features'][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Foldset.to_pandas()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `splitset.to_numpy(splits:list)`, the `foldset.to_numpy(fold_names:list)` argument allows you to pluck the `['folds_train_combined]` and `['fold_validation]` slices. Just make sure you remember to specific all 3 levels of keys when accessing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      species\n",
       "1042        2\n",
       "1043        2\n",
       "1044        2\n",
       "1045        2\n",
       "1046        2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset.to_pandas(\n",
    "    fold_index = 0\n",
    "\t, fold_names = ['folds_train_combined']\n",
    "\t, include_label = True\n",
    "\t, include_featureset = False\n",
    ")[0]['folds_train_combined']['labels'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optionally, stage an `Encoderset` for encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain algorithms either (a) require features and/ or labels formatted a certain way, or (b) perform MUCH better when their values are normalized. For example:\n",
    "\n",
    "* Converting ordinal or categorical string data `[dog, cat, fish]` into one-hot encoded format `[[1,0,0][0,1,0][0,0,1]]`.\n",
    "* Scaling continuous features from (-1 to 1) or (0.0 to 1.0). Or transforming them to resemble a more Gaussian distribution.\n",
    "\n",
    "There are two phases of encoding:\n",
    "1. `fit` - where the encoder learns about the values of the samples made available to it. Ideally, you only want to `fit` aka learn from your training split so that you are not *\"leaking\"* information from your validation and test spits into your encoder!\n",
    "2. `transform` - where the encoder transforms all of the samples in the population.\n",
    "\n",
    "AIQC has solved the following challenges related to encoding:\n",
    "\n",
    "* How does one dynamically `fit` on only the training samples in advanced scenarios like cross-validation where a different fold is used for validation each time?\n",
    "\n",
    "* For certain encoders, especially categorical ones, there is arguably no leakage. If an encoder is arbitrarilly assigning values/ tags to a sample through a process that is not aggregate-informed, then the information that is reveal to the `fit` is largely irrelevant. As an analogy, if we are examining swan color and all of a sudden there is a black swan... it's clearly not white, so slap a non-white label on it and move on. In fact, the prediction process and performance metric calucatlion may fail if it doesn't know how to handle the previously unseen category.\n",
    "\n",
    "* Certain encoders only accept certain dtypes. Certain encoders only accept certain dimensionality (e.g. 1D, 2D, 3D) or shape patterns (odd-by-odd square). Unfortunately, there is not much uniformity here.\n",
    "\n",
    "* Certain encoders output extraneous objects that don't work with deep learning libraries.\n",
    "\n",
    "> *For now, only `sklearn.preprocessing` methods are supported. That may change as we add support for more low-level tensor-based frameworks like PyTorch.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping this in mind, we create an `Encoderset` for our `Splitset`. We can attach a `Labelcoder` and/ or `Featurecoder`(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderset = splitset.make_encoderset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then import any scikit-learn encoders that you need. AIQC only supports the uppercase methods (e.g. `RobustScaler`, but not `robust_scale`) because the lowercase methods do not separate the `fit` and `transform` steps. FYI, most of the uppercase methods have a combined `fit_transform` method if you need them. \n",
    "\n",
    "> https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optionally, set a single `Labelcoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplistic `Labelcoder` is a good warmup for the moe advanced `Featurecoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you cannot encode Labels if your `Splitset` does not have labels in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is straightforward. You provide an instantiated encoder [e.g. `StandardScaler()` not `StandardScaler`], and then AIQC will:\n",
    "\n",
    "* Verify that the encoder works with your `Label`'s dtype, sample values, and figure out what dimensionality it needs in order to succeed.\n",
    "\n",
    "* Validate the attributes of your encoder to smooth out any common errors they would cause.\n",
    "\n",
    "* Determine whether the encoder should be `fit` either (a) exclusively on the train split, or (b) if it is not prone to leakage, inclusively on the entire dataset thereby reducing the chance of errors arising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcoder = encoderset.make_labelcoder(\n",
    "    sklearn_preprocess = OneHotEncoder(sparse=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optionally, determine a sequence of `Featurecoder`(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Featurecoder` has the same validation process as the `Labelcoder`. However, it is not without its own challenges:\n",
    "\n",
    "* We want to be able to apply different encoders to features of different dtypes. So it's likely that the same encoder will neither be applied to all columns, nor will all encoders be applied at the same exact time.\n",
    "\n",
    "* Additionally, even within the same dtype (e.g. float/ continuous), different distributions call for different encoders.\n",
    "\n",
    "* Commonly used encoders such a `OneHotEncoder` can ouput multiple columns from a single column input. Therefore, the structure of the features columns is not fixed during encoding.\n",
    "\n",
    "* And finally, throughout this entire process, we need to avoid data leakage.\n",
    "\n",
    "For these reasons, `Featurecoder`'s are applied sequentially; in an ordered chain, one after the other. After an encoder is applied, its columns are removed from the raw featureset and placed into an intermediary cache specific to each split/ fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Right now, `Featurecoder` cannot be created for `Dataset.Image.Featureset`. I'm not opposed to changing this, but I would just have to account for 3D arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering mode is either:\n",
    "\n",
    "* Inclusive (`include=True`) encode columns that match the filter.\n",
    "\n",
    "* Exclusive (`include=False`) encode columns outside of the filter.\n",
    "\n",
    "Then you can select:\n",
    "\n",
    "1. An optional list of `dtypes`.\n",
    "\n",
    "2. An optional list of `columns` name.\n",
    "\n",
    "  * The column filter is applied after the dtype filter. \n",
    "  \n",
    "> You can create a filter for all columns by setting `include=False` and then seting both `dtypes` and `columns` to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting your encoder, if `verbose=True` is enabled:\n",
    "* The validation rules help determine why it may have failed.\n",
    "* The print statements help determine which columns your current filter matched, and which raw columns remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___/ featurecoder_index: 0 \\_________\n",
      "\n",
      "=> The column(s) below matched your filter(s) and were ran through a test-encoding successfully.\n",
      "['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\n",
      "=> Nice! Now all feature column(s) have encoder(s) associated with them.\n",
      "No more Featurecoders can be added to this Encoderset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurecoder = encoderset.make_featurecoder(\n",
    "    sklearn_preprocess = PowerTransformer(method='yeo-johnson', copy=False)\n",
    "    , include = True\n",
    "    , dtypes = ['float64']\n",
    "    , columns = None\n",
    "    , verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view this information via the following attributes: `matching_columns`, `leftover_dtypes`, and `leftover_columns`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create an `Algorithm` aka model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been prepared, we transition to the other half of the ORM where the focus is the logic that will be applied to that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An `Algorithm` is the ORM's codename for a machine learning model since *Model* is the most important *reserved word* for ORMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attributes tell AIQC how to handle the Algorithm behind the scenes:\n",
    "\n",
    "* `library` - right now, only 'keras' is supported.\n",
    "\n",
    "  * Each library's model object and callbacks (history, early stopping) need to be handled differently.\n",
    "  \n",
    "  \n",
    "* `analysis_type` - right now, these types are supported:\n",
    "\n",
    "  * `'classification_multi'`, `'classification_binary'`, `'regression'`.\n",
    "  \n",
    "  * Used to determine which performance metrics to run.\n",
    "  \n",
    "  * Must be compatible with the type of label fed to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Algorithm` is composed of the functions:\n",
    "\n",
    "* `fn_build`.\n",
    "\n",
    "* `fn_lose` (optional, inferred).\n",
    "\n",
    "* `fn_optimize` (optional, inferred).\n",
    "\n",
    "* `fn_train`.\n",
    "\n",
    "* `fn_predict` (optional, inferred).\n",
    "\n",
    "> May provide overridable defaults for build and train in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can name the functions whatever you want, but do not change the predetermined arguments (e.g. `input_shape`,`**hp`, `model`, etc.) or their position.\n",
    "\n",
    "As we define these functions, we'll see that we can pass a dictionary of *hyperparameters* into these function using the `**hp` kwarg, and access them like so: `hp['<some_variable_name>']`. Later, we'll provide a list of values for each entry in the hyperparameters dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the modules that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Later, when running your `Job`'s, if you receive a \"module not found\" error, then you can try troubleshooting by importing that module directly within the function where it is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build your topology however you like, just be sure to `return model`. Also, you don't have to use any of the hyperparameters (`**hp`) if you don't want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatically provided `features_shape` and `label_shape` are handy because:\n",
    "\n",
    "* The number of feature/ label columns is mutable due to encoders (e.g. OHE). \n",
    "\n",
    "* Shapes can be less obvious in multi-dimensional scenarios like colored images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can customize the metrics if you so desire (e.g. change the loss or accuracy), but they will only be applied to the training process/ `History` callback. We'll see later that AIQC will calculate metrics for you automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_build(features_shape, label_shape, **hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp['neuron_count'], input_shape=features_shape, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(units=label_shape[0], activation='softmax'))\n",
    "    #optimizer and loss defined separately.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional, function to calculate loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just specify the loss function in our training loop because we will need it later on when it comes time to produce metrics about other splits/ folds.\n",
    "\n",
    "If you do not provide an `fn_lose` then one will be automatically selected for you based on the `Algorithm.analysis_type` you are conducting and the `Algorithm.library` you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_lose(**hp):\n",
    "    loser = keras.losses.CategoricalCrossentropy()\n",
    "    return loser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional, function to optimize model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some deep learning libraries persist their model and optimizer separately during checkpoint/exporting. So `fn_optimize` provides an isolated way to access the optimizer. It also allows us to automatically set the optimizer.\n",
    "\n",
    "If you do not provide an `fn_optimize` then one will be automatically selected for you based on the `Algorithm.analysis_type` you are conducting and the `Algorithm.library` you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimize(**hp):\n",
    "    optimizer = keras.optimizers.Adamax(learning_rate=0.01)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you want to define your own optimizer, then you should do so within this function, rather than relying on `model.compile(optimizer='<some_optimizer_name>'`. If you do not define an optimizer, then `Adamax` will be used by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `samples_train` - the appropriate data will be fed into the training cycle. For example, `Foldset.samples[fold_index]['folds_train_combined']` or `Splitset.samples['train']`.\n",
    "\n",
    "* `samples_evaluate` - the appropriate data is made available for evaluation. For example, `Foldset.samples[fold_index]['fold_validation']`, `Splitset.samples['validation']`, or `Splitset.samples['test']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):\n",
    "    model.compile(\n",
    "        loss = loser\n",
    "        , optimizer = optimizer\n",
    "        , metrics = ['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        samples_train[\"features\"]\n",
    "        , samples_train[\"labels\"]\n",
    "        , validation_data = (\n",
    "            samples_evaluate[\"features\"]\n",
    "            , samples_evaluate[\"labels\"]\n",
    "        )\n",
    "        , verbose = 0\n",
    "        , batch_size = 3\n",
    "        , epochs = hp['epoch_count']\n",
    "        , callbacks=[History()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional, callback to stop training early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Early stopping* isn't just about efficiency in reducing the number of `epochs`. If you've specified 300 epochs, there's a chance your model catches on to the underlying patterns early, say around 75-125 epochs. At this point, there's also good chance what it learns in the remaining epochs will cause it to overfit on patterns that are specific to the training data, and thereby and lose it's simplicity/ generalizability.\n",
    "\n",
    "> The `val_` prefix refers to the evaluation samples.\n",
    ">\n",
    "> Remember, regression does not have accuracy metrics.\n",
    ">\n",
    "> `TrainingCallback.Keras.MetricCutoff` is a custom class we wrote to make multi-metric cutoffs easier, so you won't find information about it in the official Keras documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):\n",
    "    model.compile(\n",
    "        loss = loser\n",
    "        , optimizer = optimizer\n",
    "        , metrics = ['accuracy']\n",
    "    )\n",
    "        \n",
    "    #Define one or more metrics to monitor.\n",
    "    metrics_cuttoffs = [\n",
    "        {\"metric\":\"val_accuracy\", \"cutoff\":0.9, \"above_or_below\":\"above\"},\n",
    "        {\"metric\":\"val_loss\", \"cutoff\":0.2, \"above_or_below\":\"below\"}\n",
    "    ]\n",
    "    cutoffs = aiqc.TrainingCallback.Keras.MetricCutoff(metrics_cuttoffs)\n",
    "    # Remember to append `cutoffs` to the list of callbacks.\n",
    "    callbacks=[History(), cutoffs]\n",
    "    \n",
    "    # No changes here.\n",
    "    model.fit(\n",
    "        samples_train[\"features\"]\n",
    "        , samples_train[\"labels\"]\n",
    "        , validation_data = (\n",
    "            samples_evaluate[\"features\"]\n",
    "            , samples_evaluate[\"labels\"]\n",
    "        )\n",
    "        , verbose = 0\n",
    "        , batch_size = 3\n",
    "        , epochs = hp['epoch_count']\n",
    "        , callbacks = callbacks\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional, function to predict samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fn_predict` will be generated for you automatically if set to `None`. The `analysis_type` and `library` of the Algorithm help determine how to handle the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Regression default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict(model, samples_predict):\n",
    "    predictions = model.predict(samples_predict['features'])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Classification binary default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classification `predictions`, both mutliclass and binary, must be returned in ordinal format. \n",
    "\n",
    "> For most libraries, classification algorithms output *probabilities* as opposed to actual predictions when running `model.predict()`. We want to return both of these object `predictions, probabilities` (the order matters) to generate performance metrics behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict(model, samples_predict):\n",
    "    probabilities = model.predict(samples_predict['features'])\n",
    "    # This is the official keras replacement for binary classes `.predict_classes()`\n",
    "    # It returns one array per sample: `[[0][1][0][1]]` \n",
    "    predictions = (probabilities > 0.5).astype(\"int32\")\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Classification multiclass default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict(model, samples_predict):\n",
    "    import numpy as np\n",
    "    probabilities = model.predict(samples_predict['features'])\n",
    "    # This is the official keras replacement for multiclass `.predict_classes()`\n",
    "    # It returns one ordinal array per sample: `[[0][2][1][2]]` \n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group the functions together in an `Algorithm`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = aiqc.Algorithm.make(\n",
    "    library = \"keras\"\n",
    "\t, analysis_type = \"classification_multi\"\n",
    "\t, fn_build = fn_build\n",
    "\t, fn_train = fn_train\n",
    "    , fn_optimize = fn_optimize # Optional\n",
    "\t, fn_predict = fn_predict # Optional\n",
    "\t, fn_lose = fn_lose # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <!> Remember to use `make` and not `create`. Deceptively,  `create` runs because it is a standard, built-in ORM method. However, it does so without any validation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optional, associate a `Hyperparamset` with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hyperparameters` below will be automatically fed into the functions above as `**kwargs` via the `**hp` argument we saw earlier.\n",
    "\n",
    "For example, wherever you see `hp['neuron_count']`, it will pull from the *key:value* pair `\"neuron_count\": [9, 12]` seen below. Where \"model A\" would have 9 neurons and \"model B\" would have 12 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "\t\"neuron_count\": [12]\n",
    "\t, \"epoch_count\": [30, 60]\n",
    "    , \"learning_rate\": [0.01, 0.03]\n",
    "}\n",
    "\n",
    "hyperparamset = aiqc.Hyperparamset.from_algorithm(\n",
    "\talgorithm_id = algorithm.id\n",
    "\t, hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The number of unique combinations escalates quickly, so in the future, we will provide different strategies for generating and selecting parameters to experiment with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Hyperparamcombo` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unique combination of hyperparameters is recorded as a `Hyperparamcombo`.\n",
    "\n",
    "Ultimately, a training `Job` will be constructed for each unique combinanation of hyperparameters aka `Hyperparamcombo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparamset.hyperparamcombo_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neuron_count': 12, 'epoch_count': 30, 'learning_rate': 0.01}\n",
      "{'neuron_count': 12, 'epoch_count': 30, 'learning_rate': 0.03}\n",
      "{'neuron_count': 12, 'epoch_count': 60, 'learning_rate': 0.01}\n",
      "{'neuron_count': 12, 'epoch_count': 60, 'learning_rate': 0.03}\n"
     ]
    }
   ],
   "source": [
    "hyperparamcombos = hyperparamset.hyperparamcombos\n",
    "\n",
    "for h in hyperparamcombos:\n",
    "    print(h.hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neuron_count</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>epoch_count</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning_rate</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           param  value\n",
       "0   neuron_count  12.00\n",
       "1    epoch_count  30.00\n",
       "2  learning_rate   0.01"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparamcombos[0].get_hyperparameters(as_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a `Queue` of training `Jobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Queue` is the central object of the \"logic side\" of the ORM. It ties together everything we need for training and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = aiqc.Queue.from_algorithm(\n",
    "\talgorithm_id = algorithm.id\n",
    "\t, splitset_id = splitset.id\n",
    "\t, hyperparamset_id = hyperparamset.id # Optional.\n",
    "\t, foldset_id = None # Optional.\n",
    "\t, encoderset_id = encoderset.id # Optional.\n",
    "    , repeat_count = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `repeat_count:int` allows us to run the same `Job` multiple times. Normally, each `Job` has 1 `Result` object associated with it upon completion. However, when `repeat_count` (> 1 of course) is used, a single `Job` will have multiple `Results`.\n",
    "\n",
    "> Due to the fact that training is a *nondeterministic* process, we are likely to get different results each time we train a model, even if we use the same set of parameters. Perhaps you've have the right topology and parameters, but, this time around, the model just didn't recgonize the patterns. Similar to flipping a coin, there is a degree of chance in it, but the real trend averages out upon repetition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `hide_test:bool` excludes the test split from the performance metrics and visualizations. This avoids data leakage by forcing the user to make decisions based on the performance on their model on the training and evaluation samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Job` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Job` in the Queue represents a `Hyperparamcombo` that needs to be trained.\n",
    "\n",
    "> If a `Foldset` is used during `Queue` creation, then (a) the number of jobs is multiplied by the `hyperparamcombo_count` and the `fold_count`, (b) each Job will have a `Fold`. Additionally, a superficial `Jobset` will be used to keep track of all Jobs related to that Foldset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`poll_statuses(as_pandas:bool=False)` is used to determine which Job-repeats have been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>repeat_index</th>\n",
       "      <th>result_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    job_id  repeat_index result_id\n",
       "0        1             0      None\n",
       "1        2             0      None\n",
       "2        3             0      None\n",
       "3        4             0      None\n",
       "4        1             1      None\n",
       "5        2             1      None\n",
       "6        3             1      None\n",
       "7        4             1      None\n",
       "8        1             2      None\n",
       "9        2             2      None\n",
       "10       3             2      None\n",
       "11       4             2      None"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue.poll_statuses(as_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute all `Jobs`.\n",
    "\n",
    "There are two ways to execute a Queue of Jobs:\n",
    "\n",
    "#### 1. `queue.run_jobs(in_background=False)`\n",
    "\n",
    "* Jobs are simply ran on a loop on the main *Process*.\n",
    "\n",
    "* Stop the Jobs with a keyboard interrupt e.g. `ctrl+Z/D/C` in Python shell or `i,i` in Jupyter.\n",
    "\n",
    "* It is the more reliable approach on Win/Mac/Lin.\n",
    "\n",
    "* Although this locks your main process (can't write more code) while models train, you can still fire up a second shell session or notebook.\n",
    "\n",
    "* Prototype your training jobs in this method so that you can see any errors that arise in the console.\n",
    "\n",
    "\n",
    "#### 2. `queue.run_jobs(in_background=True)`; experimental\n",
    "\n",
    "* The Jobs loop is executed on a separate, parallel `multiprocessing.Process`\n",
    "\n",
    "* Stop the Jobs with `queue.stop_jobs()`, which kills the parallel *Process* unless it already failed.\n",
    "\n",
    "* The benefit is that you can continue to code while your models are trained. There is no performance boost.\n",
    "\n",
    "* On Mac and Linux (Unix), `'fork'` multiprocessing is used (`force=True`), which allows us to display the progress bar. FYI, even in 'fork' mode, Python multiprocessing is much more fragile in Python 3.8, which seems to be caused by how pickling is handled in passing variables to the child process.\n",
    "\n",
    "* On Windows, `'spawn'` multiprocessing is used, which requires polling:\n",
    "\n",
    "  * `queue.poll_statuses()`\n",
    "  \n",
    "  * `queue.poll_progress(raw:bool=False, loop:bool=False, loop_delay:int=3)` where `raw=True` is just a float, `loop=True` won't stop checking jobs until they are all complete, and `loop_delay=3` checks the progress every 3 seconds. \n",
    "  \n",
    "* It is a known bug that the `aiqc.TrainingCallbacks.Keras.MetricCutoff` class does not work with `in_background=True` as of Python 3.8.\n",
    "\n",
    "* Also, during stress tests, I observed that when running multiple queues at the same time, the SQLite database would lock when simultaneous writes were attempted.\n",
    "\n",
    "#### 3. Future, distributed cloud execution.\n",
    "\n",
    "* In the future, we look to provide options for horizontal and vertical scale via either AWS or Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔮 Training Models 🔮: 100%|████████████████████████████████████████| 12/12 [00:49<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "queue.run_jobs(in_background=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queue is interuptable. You can stop the execution of a queue and resume it later.\n",
    "\n",
    "> This also comes in handy if either your machine or Python kernel either crashes or are interupted by accident. Whatever the reason, rest easy, just `run_jobs()` again to pick up where you left off. Be aware that the `tqdm` iteration time in the progress bar will be wrong because it will be divided by the jobs already ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Assess the `Results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Job` has a `Result`. The following attributes are automatically written to the `Result` after training.\n",
    "    \n",
    "* `model_file`: HDF5 bytes of the model.\n",
    "\n",
    "* `history`: per epoch metrics recorded during training.\n",
    "\n",
    "* `predictions`: dictionary of predictions per split/ fold.\n",
    "\n",
    "* `probabilities`: dictionary of prediction probabilities per split/ fold.\n",
    "\n",
    "* `metrics`: dictionary of single-value metrics depending on the analysis_type.\n",
    "\n",
    "* `plot_data`: metrics readily formatted for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dictionary attributes use split/ fold-based keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x14ac08460>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_model = queue.jobs[0].results[0].get_model()\n",
    "compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': {'accuracy': 0.9666666666666667,\n",
       "  'f1': 0.9665831244778613,\n",
       "  'loss': 0.13921520113945007,\n",
       "  'precision': 0.9696969696969696,\n",
       "  'recall': 0.9666666666666667,\n",
       "  'roc_auc': 0.9966666666666666},\n",
       " 'validation': {'accuracy': 0.9444444444444444,\n",
       "  'f1': 0.9440559440559441,\n",
       "  'loss': 0.22046174108982086,\n",
       "  'precision': 0.9523809523809523,\n",
       "  'recall': 0.9444444444444444,\n",
       "  'roc_auc': 0.976851851851852},\n",
       " 'train': {'accuracy': 0.9607843137254902,\n",
       "  'f1': 0.9607843137254902,\n",
       "  'loss': 0.13327109813690186,\n",
       "  'precision': 0.9607843137254902,\n",
       "  'recall': 0.9607843137254902,\n",
       "  'roc_auc': 0.9969723183391003}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue.jobs[0].results[0].metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Metrics & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on visualization of performance metrics, reference the [Visualization & Metrics](visualization.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
